\documentclass{article}
\usepackage{geometry}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts,amsthm}

\theoremstyle{demo}
\newtheorem{demo}{Demo}[section]

\begin{document}
\title{Physics 4420: Computational Skills in Physics}
\maketitle
\section{Numerical Calculus}
\subsection{Differentiation}
Think back to your first course in calculus.  After finishing with limits, you
were likely finally presented with the definition of the derivative:
\begin{equation}
    \frac{df}{dx}(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}
\end{equation}

Pretty much the entire basis of numerical (computational) physics, at least when
we're talking about calculus, is taking that definition and fudging
$h\rightarrow 0$ to $h\rightarrow\textrm{small}$.  We're just going to fudge in
a careful, controlled manner :).

Let's see how this actually works, for a function we know the derivative of
(let's say sine).

\begin{demo}[diff.cpp]
    Show that the results look OK for $h=0.1$ using \textit{plotxy} and showing
    the plain numbers.  Let the class try the same thing with a function of
    their choice.
\end{demo}

That looks \textit{ok}, but not great.  You can see that the values aren't
exactly identical.  That's not surprising though, since we're \textit{supposed}
to be taking $h\rightarrow0$.  Let's see what happens when we make it closer to
zero.

\begin{demo}[diff\_converge.cpp]
    Show how the average error changes as we decrease $h$. No need to plot,
    numbers are enough.  Have the class try the same thing for their function.
\end{demo}

You can see here that the error goes down roughly linearly: halve $h$, and halve
the error.  The rate at which the error of a scheme goes down relative to the
resolution or step size is called it's \textit{convergence rate}.
But can we do better?

\subsubsection{Central Difference}
To do better, let's use a tool that, if it isn't already your best friend, will
soon become it: the Taylor Series Expansion.  Recall this is defined as:

\begin{equation}
    \begin{aligned}
        f(x) & = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2}(x-x_0)^2 + ... \\
        & = \sum_{n=0}^\infty \frac{1}{n!} \frac{d^n f}{dx^n}(x_0) (x-x_0)^n
    \end{aligned}
\end{equation}

Let's clean this up a bit to match the kind of notation that's common in
computational physics.  Let's define:

\begin{equation}
    h = x - x_0 \equiv \Delta x
\end{equation}

$\Delta x$ will be our ``step size'', a term we will become intimately familiar
with as we progress.  We can then define points recursively, where:

\begin{equation}
    x_{i+1} = x_i + \Delta x
\end{equation}

and $f(x_i) = f_i$. In this new notation, our Taylor series expansion looks
like:
\begin{equation}
    f_{i+1} = f_i + f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 + ... =
    \sum_{n=0}^\infty \frac{1}{n!} \left.\frac{d^n f}{dx^n}\right|_i \Delta x^n\\
\end{equation}

With this, we can re-arrange to get an expression not for $f_i$, but instead
$f'_i$:

\begin{equation}
    \begin{aligned}
        f'_i & = \frac{f_{i+1} - f_i}{\Delta x} - \frac{f''_i}{2}\Delta x - ...\\
    \end{aligned}
\end{equation}

Notice that the first term here is what we plugged in to our \textit{finite
difference} scheme.  That's the formal name for this method of estimating a
derivative, since what we've done is taken an exact expression for the
derivative (our Taylor series), and truncated to include a finite number of
terms.  We can also see here why the error grows linearly:  the first omitted
term is linear in $\Delta x$.

Here's a good point to introduce ``Big-O Notation''.  We're going to frequently
find ourselves facing infinite sums that depend on different powers of something
(frequently our step size $\Delta x$).  It's useful to be able to talk about
these sums just in terms of their largest term.  If we've got a parameter like
$\Delta x$ that is small, then the highest power will be the largest.  So if
that term is proportional to $\Delta x$, then the omitted term will be
$\mathcal{O}(\Delta x)$.  Note that this term is always positive, which makes
sense if you think about it: we don't know what the sign of some arbitrary
high-order derivative is, and if you think about this as an error term the
errors are usually symmetric about zero.

Thus, our finite difference can be expressed as:

\begin{equation}
    f'_i = \frac{f_{i+1} - f_i}{\Delta x} + \mathcal{O}(\Delta x)
\end{equation}

That first term is our error, and it tells us it scales linearly as $\Delta x$.

It turns out we can do better than an error that's linear in $\Delta x$ if we
are clever.  Let's look at the Taylor expansion for $f_{i+1}$ and $f_{i-1}$
(remember this would be $f(x+\Delta x)$ and $f(x-\Delta x)$):

For $f_{i+1}$, we get:
\begin{equation}
    f_{i+1} = f_i + f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 +
    \mathcal{O}(\Delta x^3)
    \label{fipone}
\end{equation}

Meanwhile, for $f_{i-1}$, we get:
\begin{equation}
    f_{i-1} = f_i - f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 +
    \mathcal{O}(\Delta x^3)
    \label{fimone}
\end{equation}

Because in this case we have $x-x_0 = -\Delta x$.  If you've had enough coffee
today, you might notice something powerful about this:  the term with $f'_i$ has
the opposite sign to the first omitted term.  So, if we subtract one term from
the other, we get:
\begin{equation}
    f_{i+1} - f_{i-1} = 2\Delta x f'_i + \mathcal{O}(\Delta x^3)
\end{equation}

Re-arranging this to solve for $f'_i$ gives us:
\begin{equation}
    f'_i = \frac{f_{i+1} - f_{i-1}}{2\Delta x}  + \mathcal{O}(\Delta x^2)
    \label{central_diff}
\end{equation}

Check that out! The error term here grows \textit{quadratically} with $\Delta
x$.  This scheme is called a \textit{central difference}.  If you think about
what this scheme is doing, it is estimating the derivative at point $x_i$ by
using information at $x_{i+1}$ and $x_{i-1}$, while \textit{ignoring} the value
at $x_i$!  It's shocking that this works!  Or does it?  Let's try it out.

\begin{demo}[diff\_central.cpp]
    Show that the central scheme has errors that scale with $\Delta x^2$.  Show
    that it converges so quickly you run out of float precision, and you need
    doubles to see further convergence.  Have the class try it on their
    function.  
\end{demo}

\subsubsection{Five-point Scheme and Stencils}
We can go even higher in accuracy by using this approach to cancel the
$\mathcal{O}(\Delta x^2)$ term.  Let's fill in that term in our expression for
$f_{i+1} - f_{i-1}$:

\begin{equation}
    f_{i+1} - f_{i-1} = 2f'_i \Delta x + 2 \frac{f^{(3)}}{6} \Delta x^3 +
    \mathcal{O}(\Delta x^5) = 2f'_i \Delta x + \frac{f^{(3)}}{3} \Delta x^3 +
    \mathcal{O}(\Delta x^5)
\end{equation}

The last term here is $\mathcal{O}(\Delta x^5)$ because of the same cancellation
that got rid of our $\mathcal{O}(\Delta x^2)$ term: any even power of $\Delta x$
will be cancelled!  So what we need now is some way to ditch the $\Delta x^3$
term.  Perhaps the same approach as before could be useful, but this time let's
look at $f_{i+2}$ and $f_{i-2}$.

\begin{equation}
    f_{i+2} = f_i + f'_i (2 \Delta x) + \frac{f''_i}{2}(2 \Delta x)^2 +
    \frac{f^{(3)}}{6} (2 \Delta x)^3 + \mathcal{O}(\Delta x^5)
\end{equation}

\begin{equation}
    f_{i-2} = f_i - f'_i (2 \Delta x) + \frac{f''_i}{2}(2 \Delta x)^2 -
    \frac{f^{(3)}}{6} (2 \Delta x)^3 + \mathcal{O}(\Delta x^5)
\end{equation}


This makes sense, since $f_{i+2}$ just means we're jumping forward from $x_i$ by
$2\Delta x$.  If we subtract these from each other, we get:

\begin{equation}
    f_{i+2} - f_{i-2} = 4f'_i \Delta x + \frac{8f^{(3)}}{3} \Delta x^3 +
    \mathcal{O}(\Delta x^5)
\end{equation}

You can probably see exactly what we need to do:  $8(f_{i+1}-f_{i-1}) -
(f_{i+2}-f_{i-2})$ will cancel the $\Delta x^3$ term.  If we plug that in, and
do some re-arranging, we will get what's called the \textit{five-point scheme}:

\begin{equation}
    f'_i = \frac{1}{12\Delta x}(f_{i-2} - 8f_{i-1} + 8f_{i+1} - f_{i+2}) +
    \mathcal{O}(\Delta x^4)
\end{equation}

\begin{demo}[diff\_fivepoint.cpp]
    Show that the five-point scheme has errors that scale with $\Delta x^4$.  
\end{demo}

This probably has you wondering: why can't we just repeat this exercise
ad-nauseum until we have a scheme that has errors always well below
floating-point roundoff?  Well, there are two downsides to using higher-order 
finite difference schemes.  The first is fairly simple: the five-point formula
requires more operations.  For a central difference, we have 2 function calls,
but the five-point requires 4.  If those are expensive, it might be cheaper to
use the central scheme.  The second issue is related to where the function is
being evaluated.  

The five-point scheme is sometimes referred to as the five-point
\textit{stencil}, because it uses five discrete positions: $x_{i-2}$ through
$x_{i+1}$.  This means we're using four neighbouring points to calculate our
derivative.  If we want to shrink our error term even more, this stencil will
need to grow.  Eventually, we'll be covering a huge fraction of the function's
domain just to calculate a derivative.  This will be very important when we
start using these tools to solve differential equations.

\subsubsection{Higher Order Derivatives and Catastrophic Cancellation}
We can use a similar approach to calculate higher-order derivatives as well.  We
probably don't want our high-order derivatives to depend on lower-order
derivatives, so let's construct a second derivative scheme for $f''_i$ that
depends only on $f$.  If we go back to equation~\ref{fipone} and
equation~\ref{fimone}, we can see how to remove the first derivative: just add
'em!

\begin{equation}
    f_{i+1} + f_{i-1} = 2f_i + f''_i \Delta x^2 + \mathcal{O}(\Delta x^3)
\end{equation}

So our second-derivative scheme will be:

\begin{equation}
    f''_i = \frac{f_{i-1} - 2f_{i} + f_{i+1}}{\Delta x^2} + \mathcal{O}(\Delta
    x)
\end{equation}

Let's see if it works.

\begin{demo}[second\_diff.cpp]
    Show that the results look OK for $h=0.1$ using \textit{plotxy} and showing
    the plain numbers.  Let the class try the same thing with a function of
    their choice.
\end{demo}

This scheme should also give us linear convergence as $\Delta x$ shrinks. Let's
test that just to be sure.
\begin{demo}[second\_convergence.cpp]
    Show that the results \textit{sort of} converge, but not very well.  Try it
    again with doubles.  Why does it not work?
\end{demo}

What the heck is going on here?  It turns out we have been playing somewhat
fast-and-loose: we've assumed that real numbers and floats are the same thing.
But they aren't.  What we've seen here is a case of \textit{catastrophic
cancellation}.  Recall that the way floats are stored gives us $\sim7$ digits of
precision for single-precision (32 bit) and $\sim16$ digits of precision for
doubles (64 bit).  To calculate the second derivative, we've added together two
numbers, and subtracted a third as part of our calculation:
\begin{equation}
    f_{i-1} - 2f_{i} + f_{i+1}
\end{equation}

Let's look closely at what happens when we subtract two numbers that have errors
associated with them (in our case, this is due to the finite precision that is
unavoidable with storing real numbers as finite, floating-point numbers).  If we
have two numbers, $\tilde\alpha$ and $\tilde\beta$, each with errors relative to their
true values $\alpha$ and $\beta$ that are
$\tilde\alpha = \alpha + \mathcal{O}(\epsilon \alpha)$ and  $\tilde\beta = \beta
+ \mathcal{O}(\epsilon \beta)$ respectively (where $\epsilon$ is some small
number), then

\begin{equation}
    \tilde \alpha - \tilde \beta = \alpha + \mathcal{O}(\epsilon \alpha) - \beta +
    \mathcal{O}(\epsilon \beta) 
\end{equation}

What happens if the true value of $\alpha$ and $\beta$ are close?  In the most
extreme case, where $\alpha = \beta$, the fractional difference between the true
value and the calculated value will become
\begin{equation}
    \frac{\tilde \alpha - \tilde \beta}{\alpha - \beta}  =
    \frac{\mathcal{O}(\epsilon \alpha)}{0} = \infty
\end{equation}

What this warns us about is that when we try to subtract floating point numbers,
if the difference between those numbers is small compared to the overall
precision, we may end up growing the error in the floating point representation
well beyond what we might expect.

\begin{demo}[cancel.cpp]
    Show how catastrophic cancellation can give a result with errors much larger
    than 1 part in $10^7$.
\end{demo}

\subsection{Integration}
Calculating the integral of a function numerically is similar to how we started
calculating numerical derivatives.  We'll start with the way you were likely
first introduced to the derivative, as a Riemann sum:

\begin{equation}
    \int_a^b f(x) dx = \lim_{N \to \infty} \sum_{i=1}^N f\left(a +
    \frac{i(b-a)}{N}\right) \frac{b-a}{N}
\end{equation}

If we once again don't let $N\rightarrow\infty$, but instead just use a ``big''
value for N, we can approximate our integral as:
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N f_i \Delta x
\end{equation}

Where $\Delta x = (b-a)/N$.  Let's see how this works in practice.

\begin{demo}[int.cpp]
    Show how well the plain old rectangle rule works.  Notice that the error
    looks like it gets worse as $x$ increases.  Why is that?  Get the class to
    try their own function.
\end{demo}

Just like with numerical differentiation, we can be more clever about how we do
this in order to reduce the error and improve the convergence.  You've probably
already seen some of these in your Calculus classes in the past.  These methods
are called the Newton-Cotes quadrature rules. We can first try to improve the
estimation of the each component by using the trapezoid Rule:
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N (f_i + f_{i+1}) \frac{\Delta x}{2}
\end{equation}

We can improve things further by using a parabolic section (aka Simpson's rule):
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N (f_i + 4f_{i+1/2}+f_{i+1}) \frac{\Delta x}{6}
\end{equation}

Let's see how these improve the estimate.

\begin{demo}[quad.cpp]
    Show how the error is much better with the trapezoid rule, and better still
    with Simpson's rule.  Have the class verify this with their function.
\end{demo}

Calculating the convergence rate for these quadrature rules is doable
analyticaly, but it's a bit tedious.  This is a good time to introduce the
\textit{numerical experiment}.  Let's find out how quickly these methods
converge empirically, by just trying a couple of different values of $N$:

\begin{demo}[converge.cpp]
    Demonstrate that the error for the rectangle rule is $\mathcal{O}(\Delta
    x)$, $\mathcal{O}(\Delta x^2)$ for the trapezoid rule, and
    $\mathcal{O}(\Delta x^4)$ for Simpson's rule.  Have the class verify this
    with their function.
\end{demo}

\subsection{Root Finding}
A common task we often find ourselves facing is trying to find the roots of a
function, that is the value of $x$ where
\begin{equation}
    f(x) = 0
\end{equation}

This problem is more useful than just the straight forward ``when's the function
equal to zero''.  We can use a root finder to find when two equations are equal,
as well as extrema (by looking at their derivatives).  These root finding
methods work in part by leveraging some of the numerical calculus tricks we've
built up previously.  All of these root finding techniques require either one or
two initial guesses, and will give us roots with a specified precision
(something often called ``tolerance'', our tolerance for error in other words)

In this section, we're going to examine a couple of different methods for
evaluating the root of a function.  To make things fun, let's pick a function
that doesn't have an analytic solution for the root:

\begin{equation}
    f(x) = e^x\ln(x)-x^2
\end{equation}

\subsubsection{Bisection Method}
The simplest root finding technique is the Bisection method, which is a kind of
``divide-and-conquer'' algorithm.  All we need for the bisection method is to
provide a value to the left and to the right of our root (bounding it, in other
words).  If we call these values $x_l$ and $x_r$, such that $f(x_l) < 0$ and
$f(x_r) > 0$, we can define a simple algorithm that will get us closer and
closer to the true root until we're within a given tolerance $\epsilon$.

The basic pseudocode of this method is as follows:
\begin{enumerate}
    \item Define a midpoint $x_m = \frac{x_l + x_r}{2}$.
    \item If $f(x_m) = 0 \pm \epsilon$, stop.  $x_m$ is the root.
    \item If $f(x_m) > 0$, then set $x_r = x_m$ and go to 1.
    \item If $f(x_m) < 0$, then set $x_l = x_m$ and go to 1.
\end{enumerate}

\begin{demo}[bisect.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~20 iterations or so.
    Show that decreasing the tolerance gives less iterations, and increasing it
    gives more.  Show that you can't go arbitrarily high.  Have the class try a
    different function.
\end{demo}
\subsubsection{Newton's Method}
The Bisection method is great, and you'll probably find yourself using it as
your default because it is very simple, and requires only some knowledge of the
vague area that the root might be in.  But we can make an algorithm that's a lot
faster if we know the derivative of the function.

With the derivative of a function, we can take a guess about a point
\textit{near} the root, and then calculate the derivative at that point.  By
linearly extrapolating to where that derivative is zero, we can iteratively
converge on a root a bit faster than with bisection. This method is called
Newton's Method, or the Newton-Raphson method.

Newton's method relies on making an initial guess as to what the root is.  This
guess $x_0$ may not be the true root, differing by some error $\delta x$ such
that $f(x_0+\delta x) = 0$.  If we Taylor expand this about $x_0$ and keep only
terms linear in $\delta x$, we get:
\begin{equation}
    \begin{aligned}
        f(x_0 + \delta x) & = f(x_0) + f'(x_0)\delta x + \mathcal{O}(\delta x^2) = 0\\
        0 &\approx f(x_0) + f'(x_0)\delta x
    \end{aligned}
\end{equation}

If you re-arrange this, you get $\delta x \approx -f(x_0)/f'(x_0)$.  With this,
we can build an \textit{iterative} method, replacing $x_0$ with $x_0 + \delta x$
until we are within our tolerance.

The basic pseudocode of this method is as follows:
\begin{enumerate}
    \item Take an initial guess $x_0$ that is near the root.
    \item If $f(x_0) = 0 \pm \epsilon$, stop.  $x_0$ is the root.
    \item Set $x_0 = x_0 - f(x_0)/f'(x_0)$ and go to 2.
\end{enumerate}

\begin{demo}[newton.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~4 iterations if we
    guess the midpoint of $[1,2]$, and ~6 if we use the edges.  Show that you
    can't go arbitrarily high.  Have the class try a different function.
\end{demo}

\subsubsection{Secant Method}
Newton's method is clearly faster at converging than the bisection method, but
it has the huge disadvantage of needing to know the derivative of a function.
Luckily, we have constructed a whole stable of schemes for calculating these
derivatives numerically.  We can just substitute these in to our algorithm in
place of the analytic derivative.  That's why the \textit{Secant method} is
sometimes called the \textit{discrete Newton method}.  

In Newton's method, we iterate $k$ times over our guess, $x_k$ and the value of
the function at this point $f_k$ and it's derivative $f'_k$.  The recursive
definition of this relationship is:
\begin{equation}
    x_{k+1} = x_k - \frac{f_k}{f'_k}
\end{equation}

We can replace the exact $f'_k$ with a finite difference approximation:
\begin{equation}
    f'_k \approx \frac{f_k - f_{k-1}}{x_k-x_{k-1}}
\end{equation}

This gives us the definition of the Secant method's guess:
\begin{equation}
    x_{k+1} = x_k - f_k\frac{x_k-x_{k-1}}{f_k-f_{k-1}}
\end{equation}

The algorithm is then essentially the same, with the added caveat that we need
to store the previous guess, and start with \textit{two} guesses.
\begin{enumerate}
    \item Take two initial guesses $x_0$ and $x_1$ that are near the root.
    \item If $f(x_0) = 0 \pm \epsilon$, stop.  $x_0$ is the root.
    \item Set $x_0 = x_0 - f(x_0)*(x_0-x_1)/(f(x_0)-f(x_1))$, and $x_1$ to be
        equal to the previous value of $x_0$.
    \item Go to 2.
\end{enumerate}

The Secant method doesn't converge quite as quickly as Newton's method, but it
doesn't require the calculation of an analytic derivative.

\begin{demo}[newton.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~7 iterations if we
    use initial guesses of 1 and 2.  Have the class try a different function.
\end{demo}


\end{document}
