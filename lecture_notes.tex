\documentclass{article}
\usepackage{geometry}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts,amsthm}

\theoremstyle{demo}
\newtheorem{demo}{Demo}[section]

\begin{document}
\title{Physics 4420: Computational Skills in Physics}
\maketitle
\section{Numerical Calculus}
\subsection{Differentiation}
Think back to your first course in calculus.  After finishing with limits, you
were likely finally presented with the definition of the derivative:
\begin{equation}
    \frac{df}{dx}(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}
\end{equation}

Pretty much the entire basis of numerical (computational) physics, at least when
we're talking about calculus, is taking that definition and fudging
$h\rightarrow 0$ to $h\rightarrow\textrm{small}$.  We're just going to fudge in
a careful, controlled manner :).

Let's see how this actually works, for a function we know the derivative of
(let's say sine).

\begin{demo}[diff.cpp]
    Show that the results look OK for $h=0.1$ using \textit{plotxy} and showing
    the plain numbers.  Let the class try the same thing with a function of
    their choice.
\end{demo}

That looks \textit{ok}, but not great.  You can see that the values aren't
exactly identical.  That's not surprising though, since we're \textit{supposed}
to be taking $h\rightarrow0$.  Let's see what happens when we make it closer to
zero.

\begin{demo}[diff\_converge.cpp]
    Show how the average error changes as we decrease $h$. No need to plot,
    numbers are enough.  Have the class try the same thing for their function.
\end{demo}

You can see here that the error goes down roughly linearly: halve $h$, and halve
the error.  The rate at which the error of a scheme goes down relative to the
resolution or step size is called it's \textit{convergence rate}.
But can we do better?

\subsubsection{Central Difference}
To do better, let's use a tool that, if it isn't already your best friend, will
soon become it: the Taylor Series Expansion.  Recall this is defined as:

\begin{equation}
    \begin{aligned}
        f(x) & = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2}(x-x_0)^2 + ... \\
        & = \sum_{n=0}^\infty \frac{1}{n!} \frac{d^n f}{dx^n}(x_0) (x-x_0)^n
    \end{aligned}
\end{equation}

Let's clean this up a bit to match the kind of notation that's common in
computational physics.  Let's define:

\begin{equation}
    h = x - x_0 \equiv \Delta x
\end{equation}

$\Delta x$ will be our ``step size'', a term we will become intimately familiar
with as we progress.  We can then define points recursively, where:

\begin{equation}
    x_{i+1} = x_i + \Delta x
\end{equation}

and $f(x_i) = f_i$. In this new notation, our Taylor series expansion looks
like:
\begin{equation}
    f_{i+1} = f_i + f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 + ... =
    \sum_{n=0}^\infty \frac{1}{n!} \left.\frac{d^n f}{dx^n}\right|_i \Delta x^n\\
\end{equation}

With this, we can re-arrange to get an expression not for $f_i$, but instead
$f'_i$:

\begin{equation}
    \begin{aligned}
        f'_i & = \frac{f_{i+1} - f_i}{\Delta x} - \frac{f''_i}{2}\Delta x - ...\\
    \end{aligned}
\end{equation}

Notice that the first term here is what we plugged in to our \textit{finite
difference} scheme.  That's the formal name for this method of estimating a
derivative, since what we've done is taken an exact expression for the
derivative (our Taylor series), and truncated to include a finite number of
terms.  We can also see here why the error grows linearly:  the first omitted
term is linear in $\Delta x$.

Here's a good point to introduce ``Big-O Notation''.  We're going to frequently
find ourselves facing infinite sums that depend on different powers of something
(frequently our step size $\Delta x$).  It's useful to be able to talk about
these sums just in terms of their largest term.  If we've got a parameter like
$\Delta x$ that is small, then the highest power will be the largest.  So if
that term is proportional to $\Delta x$, then the omitted term will be
$\mathcal{O}(\Delta x)$.  Note that this term is always positive, which makes
sense if you think about it: we don't know what the sign of some arbitrary
high-order derivative is, and if you think about this as an error term the
errors are usually symmetric about zero.

Thus, our finite difference can be expressed as:

\begin{equation}
    f'_i = \frac{f_{i+1} - f_i}{\Delta x} + \mathcal{O}(\Delta x)
\end{equation}

That first term is our error, and it tells us it scales linearly as $\Delta x$.

It turns out we can do better than an error that's linear in $\Delta x$ if we
are clever.  Let's look at the Taylor expansion for $f_{i+1}$ and $f_{i-1}$
(remember this would be $f(x+\Delta x)$ and $f(x-\Delta x)$):

For $f_{i+1}$, we get:
\begin{equation}
    f_{i+1} = f_i + f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 +
    \mathcal{O}(\Delta x^3)
    \label{fipone}
\end{equation}

Meanwhile, for $f_{i-1}$, we get:
\begin{equation}
    f_{i-1} = f_i - f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 +
    \mathcal{O}(\Delta x^3)
    \label{fimone}
\end{equation}

Because in this case we have $x-x_0 = -\Delta x$.  If you've had enough coffee
today, you might notice something powerful about this:  the term with $f'_i$ has
the opposite sign to the first omitted term.  So, if we subtract one term from
the other, we get:
\begin{equation}
    f_{i+1} - f_{i-1} = 2\Delta x f'_i + \mathcal{O}(\Delta x^3)
\end{equation}

Re-arranging this to solve for $f'_i$ gives us:
\begin{equation}
    f'_i = \frac{f_{i+1} - f_{i-1}}{2\Delta x}  + \mathcal{O}(\Delta x^2)
    \label{central_diff}
\end{equation}

Check that out! The error term here grows \textit{quadratically} with $\Delta
x$.  This scheme is called a \textit{central difference}.  If you think about
what this scheme is doing, it is estimating the derivative at point $x_i$ by
using information at $x_{i+1}$ and $x_{i-1}$, while \textit{ignoring} the value
at $x_i$!  It's shocking that this works!  Or does it?  Let's try it out.

\begin{demo}[diff\_central.cpp]
    Show that the central scheme has errors that scale with $\Delta x^2$.  Show
    that it converges so quickly you run out of float precision, and you need
    doubles to see further convergence.  Have the class try it on their
    function.  
\end{demo}

\subsubsection{Five-point Scheme and Stencils}
We can go even higher in accuracy by using this approach to cancel the
$\mathcal{O}(\Delta x^2)$ term.  Let's fill in that term in our expression for
$f_{i+1} - f_{i-1}$:

\begin{equation}
    f_{i+1} - f_{i-1} = 2f'_i \Delta x + 2 \frac{f^{(3)}}{6} \Delta x^3 +
    \mathcal{O}(\Delta x^5) = 2f'_i \Delta x + \frac{f^{(3)}}{3} \Delta x^3 +
    \mathcal{O}(\Delta x^5)
\end{equation}

The last term here is $\mathcal{O}(\Delta x^5)$ because of the same cancellation
that got rid of our $\mathcal{O}(\Delta x^2)$ term: any even power of $\Delta x$
will be cancelled!  So what we need now is some way to ditch the $\Delta x^3$
term.  Perhaps the same approach as before could be useful, but this time let's
look at $f_{i+2}$ and $f_{i-2}$.

\begin{equation}
    f_{i+2} = f_i + f'_i (2 \Delta x) + \frac{f''_i}{2}(2 \Delta x)^2 +
    \frac{f^{(3)}}{6} (2 \Delta x)^3 + \mathcal{O}(\Delta x^5)
\end{equation}

\begin{equation}
    f_{i-2} = f_i - f'_i (2 \Delta x) + \frac{f''_i}{2}(2 \Delta x)^2 -
    \frac{f^{(3)}}{6} (2 \Delta x)^3 + \mathcal{O}(\Delta x^5)
\end{equation}


This makes sense, since $f_{i+2}$ just means we're jumping forward from $x_i$ by
$2\Delta x$.  If we subtract these from each other, we get:

\begin{equation}
    f_{i+2} - f_{i-2} = 4f'_i \Delta x + \frac{8f^{(3)}}{3} \Delta x^3 +
    \mathcal{O}(\Delta x^5)
\end{equation}

You can probably see exactly what we need to do:  $8(f_{i+1}-f_{i-1}) -
(f_{i+2}-f_{i-2})$ will cancel the $\Delta x^3$ term.  If we plug that in, and
do some re-arranging, we will get what's called the \textit{five-point scheme}:

\begin{equation}
    f'_i = \frac{1}{12\Delta x}(f_{i-2} - 8f_{i-1} + 8f_{i+1} - f_{i+2}) +
    \mathcal{O}(\Delta x^4)
\end{equation}

\begin{demo}[diff\_fivepoint.cpp]
    Show that the five-point scheme has errors that scale with $\Delta x^4$.  
\end{demo}

This probably has you wondering: why can't we just repeat this exercise
ad-nauseum until we have a scheme that has errors always well below
floating-point roundoff?  Well, there are two downsides to using higher-order 
finite difference schemes.  The first is fairly simple: the five-point formula
requires more operations.  For a central difference, we have 2 function calls,
but the five-point requires 4.  If those are expensive, it might be cheaper to
use the central scheme.  The second issue is related to where the function is
being evaluated.  

The five-point scheme is sometimes referred to as the five-point
\textit{stencil}, because it uses five discrete positions: $x_{i-2}$ through
$x_{i+1}$.  This means we're using four neighbouring points to calculate our
derivative.  If we want to shrink our error term even more, this stencil will
need to grow.  Eventually, we'll be covering a huge fraction of the function's
domain just to calculate a derivative.  This will be very important when we
start using these tools to solve differential equations.

\subsubsection{Higher Order Derivatives and Catastrophic Cancellation}
We can use a similar approach to calculate higher-order derivatives as well.  We
probably don't want our high-order derivatives to depend on lower-order
derivatives, so let's construct a second derivative scheme for $f''_i$ that
depends only on $f$.  If we go back to equation~\ref{fipone} and
equation~\ref{fimone}, we can see how to remove the first derivative: just add
'em!

\begin{equation}
    f_{i+1} + f_{i-1} = 2f_i + f''_i \Delta x^2 + \mathcal{O}(\Delta x^3)
\end{equation}

So our second-derivative scheme will be:

\begin{equation}
    f''_i = \frac{f_{i-1} - 2f_{i} + f_{i+1}}{\Delta x^2} + \mathcal{O}(\Delta
    x)
\end{equation}

Let's see if it works.

\begin{demo}[second\_diff.cpp]
    Show that the results look OK for $h=0.1$ using \textit{plotxy} and showing
    the plain numbers.  Let the class try the same thing with a function of
    their choice.
\end{demo}

This scheme should also give us linear convergence as $\Delta x$ shrinks. Let's
test that just to be sure.
\begin{demo}[second\_convergence.cpp]
    Show that the results \textit{sort of} converge, but not very well.  Try it
    again with doubles.  Why does it not work?
\end{demo}

What the heck is going on here?  It turns out we have been playing somewhat
fast-and-loose: we've assumed that real numbers and floats are the same thing.
But they aren't.  What we've seen here is a case of \textit{catastrophic
cancellation}.  Recall that the way floats are stored gives us $\sim7$ digits of
precision for single-precision (32 bit) and $\sim16$ digits of precision for
doubles (64 bit).  To calculate the second derivative, we've added together two
numbers, and subtracted a third as part of our calculation:
\begin{equation}
    f_{i-1} - 2f_{i} + f_{i+1}
\end{equation}

Let's look closely at what happens when we subtract two numbers that have errors
associated with them (in our case, this is due to the finite precision that is
unavoidable with storing real numbers as finite, floating-point numbers).  If we
have two numbers, $\tilde\alpha$ and $\tilde\beta$, each with errors relative to their
true values $\alpha$ and $\beta$ that are
$\tilde\alpha = \alpha + \mathcal{O}(\epsilon \alpha)$ and  $\tilde\beta = \beta
+ \mathcal{O}(\epsilon \beta)$ respectively (where $\epsilon$ is some small
number), then

\begin{equation}
    \tilde \alpha - \tilde \beta = \alpha + \mathcal{O}(\epsilon \alpha) - \beta +
    \mathcal{O}(\epsilon \beta) 
\end{equation}

What happens if the true value of $\alpha$ and $\beta$ are close?  In the most
extreme case, where $\alpha = \beta$, the fractional difference between the true
value and the calculated value will become
\begin{equation}
    \frac{\tilde \alpha - \tilde \beta}{\alpha - \beta}  =
    \frac{\mathcal{O}(\epsilon \alpha)}{0} = \infty
\end{equation}

What this warns us about is that when we try to subtract floating point numbers,
if the difference between those numbers is small compared to the overall
precision, we may end up growing the error in the floating point representation
well beyond what we might expect.

\begin{demo}[cancel.cpp]
    Show how catastrophic cancellation can give a result with errors much larger
    than 1 part in $10^7$.
\end{demo}

\subsection{Integration}
Calculating the integral of a function numerically is similar to how we started
calculating numerical derivatives.  We'll start with the way you were likely
first introduced to the derivative, as a Riemann sum:

\begin{equation}
    \int_a^b f(x) dx = \lim_{N \to \infty} \sum_{i=1}^N f\left(a +
    \frac{i(b-a)}{N}\right) \frac{b-a}{N}
\end{equation}

If we once again don't let $N\rightarrow\infty$, but instead just use a ``big''
value for N, we can approximate our integral as:
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N f_i \Delta x
\end{equation}

Where $\Delta x = (b-a)/N$.  Let's see how this works in practice.

\begin{demo}[int.cpp]
    Show how well the plain old rectangle rule works.  Notice that the error
    looks like it gets worse as $x$ increases.  Why is that?  Get the class to
    try their own function.
\end{demo}

Just like with numerical differentiation, we can be more clever about how we do
this in order to reduce the error and improve the convergence.  You've probably
already seen some of these in your Calculus classes in the past.  These methods
are called the Newton-Cotes quadrature rules. We can first try to improve the
estimation of the each component by using the trapezoid Rule:
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N (f_i + f_{i+1}) \frac{\Delta x}{2}
\end{equation}

We can improve things further by using a parabolic section (aka Simpson's rule):
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N (f_i + 4f_{i+1/2}+f_{i+1}) \frac{\Delta x}{6}
\end{equation}

Let's see how these improve the estimate.

\begin{demo}[quad.cpp]
    Show how the error is much better with the trapezoid rule, and better still
    with Simpson's rule.  Have the class verify this with their function.
\end{demo}

Calculating the convergence rate for these quadrature rules is doable
analyticaly, but it's a bit tedious.  This is a good time to introduce the
\textit{numerical experiment}.  Let's find out how quickly these methods
converge empirically, by just trying a couple of different values of $N$:

\begin{demo}[converge.cpp]
    Demonstrate that the error for the rectangle rule is $\mathcal{O}(\Delta
    x)$, $\mathcal{O}(\Delta x^2)$ for the trapezoid rule, and
    $\mathcal{O}(\Delta x^4)$ for Simpson's rule.  Have the class verify this
    with their function.
\end{demo}

\subsection{Root Finding}
A common task we often find ourselves facing is trying to find the roots of a
function, that is the value of $x$ where
\begin{equation}
    f(x) = 0
\end{equation}

This problem is more useful than just the straight forward ``when's the function
equal to zero''.  We can use a root finder to find when two equations are equal,
as well as extrema (by looking at their derivatives).  These root finding
methods work in part by leveraging some of the numerical calculus tricks we've
built up previously.  All of these root finding techniques require either one or
two initial guesses, and will give us roots with a specified precision
(something often called ``tolerance'', our tolerance for error in other words)

In this section, we're going to examine a couple of different methods for
evaluating the root of a function.  To make things fun, let's pick a function
that doesn't have an analytic solution for the root:

\begin{equation}
    f(x) = e^x\ln(x)-x^2
\end{equation}

\subsubsection{Bisection Method}
The simplest root finding technique is the Bisection method, which is a kind of
``divide-and-conquer'' algorithm.  All we need for the bisection method is to
provide a value to the left and to the right of our root (bounding it, in other
words).  If we call these values $x_l$ and $x_r$, such that $f(x_l) < 0$ and
$f(x_r) > 0$, we can define a simple algorithm that will get us closer and
closer to the true root until we're within a given tolerance $\epsilon$.

The basic pseudocode of this method is as follows:
\begin{enumerate}
    \item Define a midpoint $x_m = \frac{x_l + x_r}{2}$.
    \item If $f(x_m) = 0 \pm \epsilon$, stop.  $x_m$ is the root.
    \item If $f(x_m) > 0$, then set $x_r = x_m$ and go to 1.
    \item If $f(x_m) < 0$, then set $x_l = x_m$ and go to 1.
\end{enumerate}

\begin{demo}[bisect.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~20 iterations or so.
    Show that decreasing the tolerance gives less iterations, and increasing it
    gives more.  Show that you can't go arbitrarily high.  Have the class try a
    different function.
\end{demo}
\subsubsection{Newton's Method}
The Bisection method is great, and you'll probably find yourself using it as
your default because it is very simple, and requires only some knowledge of the
vague area that the root might be in.  But we can make an algorithm that's a lot
faster if we know the derivative of the function.

With the derivative of a function, we can take a guess about a point
\textit{near} the root, and then calculate the derivative at that point.  By
linearly extrapolating to where that derivative is zero, we can iteratively
converge on a root a bit faster than with bisection. This method is called
Newton's Method, or the Newton-Raphson method.

Newton's method relies on making an initial guess as to what the root is.  This
guess $x_0$ may not be the true root, differing by some error $\delta x$ such
that $f(x_0+\delta x) = 0$.  If we Taylor expand this about $x_0$ and keep only
terms linear in $\delta x$, we get:
\begin{equation}
    \begin{aligned}
        f(x_0 + \delta x) & = f(x_0) + f'(x_0)\delta x + \mathcal{O}(\delta x^2) = 0\\
        0 &\approx f(x_0) + f'(x_0)\delta x
    \end{aligned}
\end{equation}

If you re-arrange this, you get $\delta x \approx -f(x_0)/f'(x_0)$.  With this,
we can build an \textit{iterative} method, replacing $x_0$ with $x_0 + \delta x$
until we are within our tolerance.

The basic pseudocode of this method is as follows:
\begin{enumerate}
    \item Take an initial guess $x_0$ that is near the root.
    \item If $f(x_0) = 0 \pm \epsilon$, stop.  $x_0$ is the root.
    \item Set $x_0 = x_0 - f(x_0)/f'(x_0)$ and go to 2.
\end{enumerate}

\begin{demo}[newton.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~4 iterations if we
    guess the midpoint of $[1,2]$, and ~6 if we use the edges.  Show that you
    can't go arbitrarily high.  Have the class try a different function.
\end{demo}

\subsubsection{Secant Method}
Newton's method is clearly faster at converging than the bisection method, but
it has the huge disadvantage of needing to know the derivative of a function.
Luckily, we have constructed a whole stable of schemes for calculating these
derivatives numerically.  We can just substitute these in to our algorithm in
place of the analytic derivative.  That's why the \textit{Secant method} is
sometimes called the \textit{discrete Newton method}.  

In Newton's method, we iterate $k$ times over our guess, $x_k$ and the value of
the function at this point $f_k$ and it's derivative $f'_k$.  The recursive
definition of this relationship is:
\begin{equation}
    x_{k+1} = x_k - \frac{f_k}{f'_k}
\end{equation}

We can replace the exact $f'_k$ with a finite difference approximation:
\begin{equation}
    f'_k \approx \frac{f_k - f_{k-1}}{x_k-x_{k-1}}
\end{equation}

This gives us the definition of the Secant method's guess:
\begin{equation}
    x_{k+1} = x_k - f_k\frac{x_k-x_{k-1}}{f_k-f_{k-1}}
\end{equation}

The algorithm is then essentially the same, with the added caveat that we need
to store the previous guess, and start with \textit{two} guesses.
\begin{enumerate}
    \item Take two initial guesses $x_0$ and $x_1$ that are near the root.
    \item If $f(x_0) = 0 \pm \epsilon$, stop.  $x_0$ is the root.
    \item Set $x_0 = x_0 - f(x_0)*(x_0-x_1)/(f(x_0)-f(x_1))$, and $x_1$ to be
        equal to the previous value of $x_0$.
    \item Go to 2.
\end{enumerate}

The Secant method doesn't converge quite as quickly as Newton's method, but it
doesn't require the calculation of an analytic derivative.

\begin{demo}[newton.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~7 iterations if we
    use initial guesses of 1 and 2.  Have the class try a different function.
\end{demo}

\section{Ordinary Differential Equations}
One of the most common kinds of problems you'll face in your career as a
physicist isn't simple differentiation or integration tasks, but instead a set
of differential (or possibly integro-differential) equations.  One of the
simplest is the ubiquitous simple harmonic oscillator.  For a SHO, we have the
following differential equations we need to solve:

\begin{equation}
    \begin{aligned}
        \frac{dv}{dt} &= -\frac{k}{m}x \\
        \frac{dx}{dt} &= v \\
    \end{aligned}
\end{equation}

For this problem, we know the solution:

\begin{equation}
    x(t) = A\cos(\omega t - \phi)
\end{equation}

Where 
\begin{equation}
    \begin{aligned}
        \omega &= \sqrt{k/m} \\
        A &= \sqrt{x_0^2 + (v_0/\omega)^2}\\
        \phi &= \arctan{\frac{v_0}{\omega x_0}} 
    \end{aligned}
\end{equation}

With initial conditions $v_0$ and $x_0$.  

But what if we \textit{don't} know the solution?  In this chapter, we're going
to learn general strategies for solving Ordinary (that is, involving only
derivatives of a single variable) Differential Equations.  We'll start with the
most common and simple type, the \textit{initial value problem}.

\subsection{Initial Value Problem}
A huge fraction of physics problems either already are first-order ODEs, or can
be transformed \textit{into} first-order ODEs.  These equations have the general
form of

\begin{equation}
    \frac{d\mathbf{y}}{dt} = \mathbf{g}(\mathbf{y}, t)
\end{equation}

Where $\mathbf{y} = (y_1, y_2, y_n)$ are some set of $n$ dynamical variables,
and $\mathbf{g}$ is a generalized velocity that governs the time evolution of
that dynamical variable.  These need not be position and velocity, they could
just as easily be energy and power, mass and mass-flux, or angle and angular
velocity.

In an initial value problem, we solve the previous problem by evolving a system
forward from some $\mathbf{y}_0=\mathbf{y}|_{t=0}$.  For our example of a SHO,
we have a dynamical variable vector:

\begin{equation}
    \mathbf{y} = 
    \begin{pmatrix}
        y_1 \\
        y_2 \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        x \\
        v \\
    \end{pmatrix}
\end{equation}

With a generalized velocity vector:
\begin{equation}
    \mathbf{g} = 
    \begin{pmatrix}
        y_2 \\
        -\omega^2 y_1 \\
    \end{pmatrix}
\end{equation}

If we have values for $x_0$ and $v_0$, we can use these together with the
equations of motion above to solve for $x(t)$ and $v(t)$.

\subsection{The Euler and Picard Methods}
There are two ways of thinking about solving an IVP.  I generally find one of
these to be the ``intuitive'' way, and the other to be the ``rigorous'' way.
Let's start with the intuitive one.  For an IVP involving position and velocity
(a kinematics problem, for example), we initially know $x_0$ and $v_0$, and we
want to know $x(t)$ (or possibly $v(t)$).  So what we want to do is ``advance''
the values of $x$ and $v$ from $t=0\rightarrow t$.  How do we do that then?
Let's imagine the simplest case, where $a=0$, so $v(t)=v_0=\mathrm{const}$.  In
this case, our solution is obvious:

\begin{equation}
    x = x_0 + vt
\end{equation}

But what if $v$ is not constant?  Well, hopefully on \textit{some} scale, it is
at least smoothly varying.  If that's the case, then maybe what we can do is
treat $v$ as constant over a short interval $\Delta t$.  Then, we can advance
our position in small jumps:

\begin{equation}
    x_{i+1} = x_i + v_i \Delta t
\end{equation}

And we can do the same thing for the velocity:
\begin{equation}
    v_{i+1} = v_i + a_i \Delta t
\end{equation}

And then to get the whole thing up to time $t$, we start with $i=0$, and advance
it to $i=t/\Delta t$, iteratively updating x and v each time.  Essentially what
we're doing here is integrating these differential equations to transform:

\begin{equation}
    \begin{aligned}
        \int \frac{dx}{dt} &\rightarrow x(t) \\
        \int \frac{dv}{dt} &\rightarrow v(t) \\
    \end{aligned}
\end{equation}

This is why these methods for solving ODEs are often called ``numerical
integration''.  The method we've derived here is the simplest numerical
integration scheme, \textit{Euler's method}.

\begin{demo}[euler.cpp]
    Show that Euler's method works for a SHO.  Show what happens when the step
    size is made too large (unstable).
\end{demo}

You can see that Euler's method works reasonably well, but errors can accumulate
over time.  Shrinking the step size helps us to shrink these errors, thankfully,
but how quickly do they shrink?  Here we're going to need to introduce the more
``rigorous'' way of deriving Euler's method:

If we take again our old friend the Taylor expansion, we can expand $y(t+\Delta
t)$ about $t$:

\begin{equation}
    \begin{aligned}
        y(t+\Delta t) &= y(t) + y'(t)\Delta t + \mathcal{O}(\Delta t^2)\\
        y_{i+1} &= y_i + g_{i}\Delta t + \mathcal{O}(\Delta t^2)\\
    \end{aligned}
\end{equation}

This gives us our Euler method, with an error estimate!  The errors should be
$\mathcal{O}(\Delta t^2)$.  Let's check that they are:

\begin{demo}[euler\_converge.cpp]
    Show that the error is only decreasing linearly with smaller step size
    $\Delta t$.  Why is that?  (Remember number of steps required is $n\propto
    \Delta t^{-1}$)
\end{demo}

We've now seen that the Euler scheme's error is linear in the size of the
timestep.  We call this convergence behaviour \textit{first-order}: it depends
on $\Delta t^1$.  Let's construct a better algorithm, a second-order integrator.

We can do a bit better than Euler by combining our approximation of a derivative
with a better approximation of an integral.  It's formally true that:

\begin{equation}
    y_{i+1} = y_i + \int_{t_i}^{t_{i+1}} g(y,t) dt
\end{equation}

(Naturally, as what I've just written out is a re-phrasing of the Fundamental
Theorem of Calculus).

Euler's method approximates this integral as:
\begin{equation}
    \int_{t_i}^{t_{i+1}} g(y,t) dt \approx g_i\Delta t
\end{equation}

(In other words, the left-hand rectangle rule).  Maybe if we use a smarter
approximation of the integral, we can get a better scheme.  Let's try the
Trapezoid rule:

\begin{equation}
    \int_{t_i}^{t_{i+1}} g(y,t) dt \approx \frac{\Delta t}{2}(g_i + g_{i+1})
\end{equation}

This should give us smaller errors than the previous case, because the trapezoid
rule has errors $\mathcal{O}(\Delta t^3)$, which should give a final scheme with
error $\mathcal{O}(\Delta t^2)$.

So now we have a scheme that looks like this:
\begin{equation}
    y_{i+1} = y_i + \frac{\Delta t}{2}(g_i+g_{i+1}) + \mathcal{O}(\Delta t^3)
\end{equation}

All well and good, except for one big question:  HOW DO WE CALCULATE SOMETHING
THAT DEPENDS ON A FUTURE VALUE?  We need $g_{i+1}$ to calculate $y_{i+1}$, but
$g_{i+1}$ presumably depends on $y_{i+1}$.  What do we do here?

What we have here is called an \textit{implicit} method/problem.  The difference
between these two methods is that an \textit{explicit} method involves finding
some function $f(y)$ such that:

\begin{equation}
    y_{i+1} = A(y_i)
\end{equation}

Whereas an implicit method instead requires you to find a root of an equation
such that:

\begin{equation}
    B(y_i, y_{i+1}) = 0
\end{equation}

In other words, an optimization or root-finding problem.  The root we need to
find is:

\begin{equation}
    y_{i+1} - y_i - \frac{\Delta t}{2}(g_i+g_{i+1}) = 0
\end{equation}

Solving for the roots $y_{i+1}$ and $g_{i+1}$ give us the next value at a given
time.  We can either use our previously derived root-finding tools for this, or
we can use a clever method called \textit{Fixed-Point Iteration}.  The idea
behind this is that we are looking to find a point $x_{fix}$ for some function,
such that:

\begin{equation}
    f(x_{fix}) = x_{fix}
\end{equation}

In our case, the fixed point is $y_{i+1}$, and the function is:

\begin{equation}
    y_{i+1} = y_i + \frac{\Delta t}{2}(g(y_i)+g(y_{i+1}))
\end{equation}

We can solve for this using one of many methods, but the easiest is almost
certainly \textit{Picard iteration}.  With this, we calculate new values for the
vector $y_{i+1}$ over and over again, eventually stopping after a fixed number
of iterations (bad! lazy!) or once a certain error tolerance has been reached.
This is defined simply as:
\begin{equation}
    \begin{aligned}
        y^{0}_{i+1} &= y_i \\
        y^{(k+1)}_{i+1} &= y_i + \frac{\Delta t}{2}(g(y_i)+g(y^{(k)}_{i+1}))
    \end{aligned}
\end{equation}

Let's see how this works with a very simple ODE:

\begin{equation}
    \frac{dy}{dt} = g = y
\end{equation}

This gives us:

\begin{equation}
    y_{i+1} = y_i + \frac{\Delta t}{2}(y_i+y_{i+1})
\end{equation}

With an initial value of $y(0) = 1$, the solution for our ODE is

\begin{equation}
    y(t) = e^t
\end{equation}

So $y(0.1)$ should be exactly $1.1051709$.  Let's see if we can Picard iterate
towards this value.  We will start from $t=0$ and go to $t=0.1$ in one big step
of $\Delta t = 0.1$.  Our initial condition will therefore be $y_i=1$, and our
initial guess will be $y^{(0)}_{i+1}=1$.  Let's see if it works.

\begin{equation}
    \begin{aligned}
        y^{(k+1)}_{i+1} &= y_i + \frac{\Delta t}{2}(g(y_i)+g(y^{(k)}_{i+1})) \\
        y^{(1)}_{i+1} &= 1 + 0.05(1+1) &= 1.01 \\
        y^{(2)}_{i+1} &= 1 + 0.05(1+1.01) &= 1.1005 \\
        y^{(3)}_{i+1} &= 1 + 0.05(1+1.1005) &= 1.105025 \\
        y^{(4)}_{i+1} &= 1 + 0.05(1+1.105025) &= 1.1052625 \\
        y^{(5)}_{i+1} &= 1 + 0.05(1+1.1052625) &= 1.10526313 \\
        y^{(6)}_{i+1} &= 1 + 0.05(1+1.10526313) &= 1.10526315 \\
    \end{aligned}
\end{equation}

You can see we've converged, but not exactly to the true value.  Remember that
we're dealing with a truncated result here: there is a non-zero error term that
just iterating away at won't help.  Let's see how well this Picard method works

\begin{demo}[picard.cpp]
    Show how much more stable the picard method is than the Euler method.  Try
    dropping the step sizes and see what happens.
\end{demo}

You've noticed this thing is \textit{way} more stable than Euler's method with
large-ish step sizes?  That's one of the huge advantages of implicit methods:
they tend to either fail utterly (like if the iteration doesn't converge) or
give a stable result.  You don't have to worry quite as much as with an
explicit method if you are going to get a result that's wrong because of a
numerical instability that's grown.  Let's see how the convergence looks.  Based
on the truncation error of $\mathcal{O}(\Delta t^3)$, the error should converge
as $\mathcal{O}(\Delta t^2)$: the method is \textit{second-order}.

\begin{demo}[picard\_converge.cpp]
    Show that the picard method is second order, but note that it is slow.  It
    takes a long time to do all those extra iterations (in our lazy case, 10
    times as much time).
\end{demo}

So we've seen now a semi-implicit method (since we're only doing implicit
calculations for intermediate steps, this is also called an IMEX or
IMplicit-EXplicit method).  Unfortuantely, all that extra iterating is
expensive.  Maybe there's a way to skip that iteration?

\subsection{Predictor-Corrector Methods}
It turns out, yes indeed there is a way to skip that iteration!  The trick is to
use a cheap, explicit first guess (like, say from the Euler method).  This
``prediction'' is then corrected using an implicit method, like the single
Picard step we saw previously.  We will first produce a
weak prediction for $\tilde y_{i+1}$:

\begin{equation}
    \tilde y_{i+1} = y_i + g_i\Delta t
\end{equation}

Which we then correct to a better estimate of $y_{i+1}$:

\begin{equation}
    y_{i+1} = y_i + \frac{\Delta t}{2}(g_i+\tilde g_{i+1})
\end{equation}

This is actually just one of many classes of ODE solvers called
``predictor-corrector'' methods.  Let's see how well it works. 
\begin{demo}[pred\_corr.cpp]
    Show that the predictor corrector is also quite stable.  Walk through the
    code.
\end{demo}

Predictor-Corrector methods can be of any order, as long as you
include enough terms.  The one we have here is second-order.  Let's verify that,
and also check that it's faster than Picard Iteration.

\begin{demo}[pc\_converge.cpp]
    Show that this is also second order, and that it's much faster than Picard.
\end{demo}

\subsection{Runge-Kutta Methods}
One of the powerful ways of building numerical (read: approximate) methods is,
as we've kept seeing, to use combinations of Taylor series approximations to
cancel out high-order error terms and leave us with a more accurate,
faster-converging method.  A general scheme was developed around the turn of the
1900s by two German mathematicians, Carl Runge and Wilhelm Kutta.  The Pang
textbook has an extremely oblique and confusing derivation of this method, so I
hope I can do a bit better.

To understand the family of Runge-Kutta methods, we're going to start out by
constructing a method that's not much better than the Predictor-Corrector or
Picard method (it's second order), but is easy to understand.  We'll start again
by expanding $y$ about $t$ and evaluating it at $t+\Delta t$:

\begin{equation}
    y(t+\Delta t) = y(t)+\Delta ty'(t) + \frac{\Delta t^2}{2}y''(t) +
    \mathcal{O}(\Delta t^3)
\end{equation}

If our derivative vector is $g(t,y) = y'(t)$, then this gives us:

\begin{equation}
    y(t+\Delta t) = y(t)+\Delta tg(t,y(t)) + \frac{\Delta t^2}{2} \frac{d}{dt}
    g(t,y(t)) + \mathcal{O}(\Delta t^3)
\end{equation}

Because $g$ is a multivariable function, we need to apply the chain rule here to
expand it:

\begin{equation}
    \frac{dg}{dt} = \frac{\partial g}{\partial t}\frac{\partial t}{\partial t} +
    \frac{\partial g}{\partial y}\frac{\partial y}{\partial t} = g_t + g_y g
\end{equation}

More formally, $\frac{\partial g}{\partial y}$ should be the Jacobian of $g$,
but we're lazy, so let's just pretend we've only got a scalar $y$.

If we then plug this into our Taylor expansion, we get:
\begin{equation}
    y(t+\Delta t) = y(t)+\Delta tg(t,y(t)) + \frac{\Delta t^2}{2}(g_t(t,y) +
    g(t,y)g_y(t,y)) + \mathcal{O}(\Delta t^3)
\end{equation}

Nothing new or too radical so far.  What we need to do now is introduce a second
Taylor series expansion, this time expanding not $y$ but instead $g$.  However,
$g$ is a multivariable function!  This means we need to introduce the
multivariate Taylor Series.

For a single variable, the Taylor series of a function $f$ expanded about $a$ is
defined as:
\begin{equation}
    f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!}(x-a)^n
\end{equation}

Where $f^{(n)}$ is the n-th order derivative of $f$.  But what do we have if $f$
depends on multiple variables?  Generally, it's a horrible mess!  I don't want
to even write it down it's such an ugly expression, look it up online if you
want to hurt your eyes.

We, unfortunately, still need to use it.  The low-order terms are pretty
reasonable and easy to understand.  Let's do that for our $g(t,y)$:

\begin{equation}
    g(t+\Delta t, y+\Delta y) = g(t,y) + g_t(t,y) \Delta t + g_y(t,y) \Delta y +
    ...
\end{equation}

If we let $\Delta y = g(t,y)\Delta t$, then we get:
\begin{equation}
    g(t+\Delta t, y+g(t,y)\Delta t) = g(t,y) + g_t(t,y) \Delta t + g_y(t,y)
    g(t,y) \Delta t + \mathcal{O}(\Delta t^2)
\end{equation}

If we re-arrange our Taylor expansion, we can see something that gives us a hint
as to what we can do with this:
\begin{equation}
    y(t+\Delta t) = y(t)+\frac{\Delta t}{2}g(t,y) + \frac{\Delta t}{2}(g(t,y) +
    g_t(t,y)\Delta t + g(t,y)g_y(t,y)\Delta t) + \mathcal{O}(\Delta t^3)
\end{equation}

The last term we didn't omit is the same as the first few terms of the Taylor
expansion for $g(t+\Delta t, y+\Delta y)$! So what we have is:

\begin{equation}
    \begin{aligned}
        y(t+\Delta t) &= y(t)+\frac{\Delta t}{2}g(t,y) + \frac{\Delta
    t}{2}\left[g(t+\Delta t, y + g\Delta t) + \mathcal{O}(\Delta t^2)\right] +
    \mathcal{O}(\Delta t^3)\\
        &= y(t)+\frac{\Delta t}{2}\left[g(t,y) + g(t+\Delta t, y + g\Delta t) \right] +
    \mathcal{O}(\Delta t^3)\\
    \end{aligned}
\end{equation}

This means, when we have $N\propto t^{-1}$ steps, the order of this method will
be $\mathcal{O}(\Delta t^2)$.  This method is called the second-order
Runge-Kutta method, or sometimes Heun's method.  Putting it into our usual
notation, it gives us:

\begin{equation}
    y_{n+1} = y_n + \Delta t(\frac{1}{2}k_1 + \frac{1}{2} k_2)
\end{equation}

Where
\begin{equation}
    \begin{aligned}
        k_1 &= g(t_n, y_n) \\
        k_2 &= g(t_n + \Delta t, y_n + k_1\Delta t) \\
    \end{aligned}
\end{equation}

Let's see if this works!
\begin{demo}[rk2.cpp]
    Show that the 2nd order RK method works, but has a slight phase offset.
\end{demo}

It should also be second order, let's check that it is:
\begin{demo}[rk2\_converge.cpp]
    Show that the 2nd order RK method is second order.
\end{demo}

Runge-Kutta methods are actually a family of methods, which all have the form:

\begin{equation}
    y_{n+1} = y_n + \Delta t\sum_{i=1}^m b_i k_i
\end{equation}

Where the terms $k_i$ are defined recursively:

\begin{equation}
    \begin{aligned}
        k_1 &= g(t_n, y_n) \\
        k_i &= g(t_n + c_i\Delta t, y_n + \Delta t\sum_{j=1}^{i-1}a_{ij}k_j)
    \end{aligned}
\end{equation}

For the second-order RK method, we have $c_2=1$, $a_{21}=1$, and $b_1=b_2=1/2$.
A convenient way of keeping track of these coefficients is the \textit{Butcher
tableau}, a notation developed by John C. Butcher in the 1970s.  The Butcher
Tableau lays out the coefficients like a matrix, which for explicit RK methods
is lower-triangular.  The Tableau looks like this:

\[
\begin{array}{c|ccccc}
    0\\
    c_2 & a_{21} \\
    c_3 & a_{31} & a_{32} \\
    \vdots & \vdots & \ddots \\
    c_m & a_{m1} & a_{m2} & \dots & a_{m,m-1} \\
    \hline
    & b_i & b_2 & \dots &  \dots & b_m \\
\end{array}
\]

We have near-total freedom to choose these parameters, except for one critical
requirement:

\begin{equation}
    \sum_{i=1}^m b_i = 1
\end{equation} 

That requirement is needed for the method to be consistent with the Taylor
series expansions that constructed it.  The choice of the other terms, and the
total number of terms to include, sets the order of the method. The RK2 method
has a Butcher Tableau of:

\[
\begin{array}{c|cccc}
    0\\
    1 & 1 \\
    \hline
    & 1/2 & 1/2 
\end{array}
\]

I won't derive it, because it's an ungodly tedious pile of work, but there is a
high order $\mathcal{O}(\Delta t^4)$ RK method that is often referred to as
``the'' Runge-Kutta Method.  RK4 has a Butcher tableau of:

\[
\begin{array}{c|cccc}
    0\\
    1/2 & 1/2 \\
    1/2 & 0 & 1/2 \\
    1 & 0 & 0 & 1 \\
    \hline
    & 1/6 & 1/3 & 1/3 & 1/6
\end{array}
\]

This ends up looking like:
\begin{equation}
    \begin{aligned}
        k1 &= g(t_n, y_n) \\
        k2 &= g(t_n+0.5\Delta t, y_n + 0.5\Delta t k_1) \\
        k3 &= g(t_n+0.5\Delta t, y_n + 0.5\Delta t k_2) \\
        k4 &= g(t_n+\Delta t, y_n + \Delta t k_3) \\
        y_{n+1} &= y_n + \frac{\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4)
    \end{aligned}
\end{equation}

In general, if you are faced with an ODE that needs solving, the RK4 method
should be the first tool you pull out.  It may not be the best choice for every
problem, but it is generally the most likely to give you a good answer on your
first try.  Let's see it in action.

\begin{demo}[rk4.cpp]
    Show that the 4nd order RK method works great.
\end{demo}

It should also be FOURTH order, let's check that it is:
\begin{demo}[rk4\_converge.cpp]
    Show that the 4nd order RK method is fourth order.
\end{demo}

\subsection{Symplectic Integrators and Energy Conservation}
So far the only criterion we've considered for whether a method for solving an
ODE is ``good'' or not is the cost per timestep $\Delta t$ as well as the
convergence: how quickly the error fell as $\Delta t$ is decreased.  However,
we saw with our example of Euler's method that the total energy actually was
increasing monotonically, which is way worse than just a random error, it's a
systematic one in a property we care about (total energy).  

It turns out that even our good friend RK4 suffers from energy non-conservation.
In our simple harmonic oscillator, the total energy (Hamiltonian) of the system
is:

\begin{equation}
    H = T + V = \frac{1}{2}(p^2 + q^2)
    \label{SHO_Hamiltonian}
\end{equation}

If we choose units $m=\omega=1$, then $x=q$ and $v=p$, which will simplify some
of our derivations.  Let's see how well the energy is conserved with a SHO and
RK4.

\begin{demo}[rk4\_energy.cpp]
    Show that the total energy of an RK4-integrated SHO decreases systematically.
\end{demo}

That's not good!  If we are dealing with a problem like an SHO, and we have many
periods/dynamical times, even RK4 is going to become a problem.  This is
extremely relevant in astrophysical problems, where things like planets can
make billions of orbits around a star during their lifetime.  We can see how bad
this can turn out by looking at an object in orbit around a point mass.  We'll
keep everything simple, and use two-dimensions.  The equations of motion for
this problem will be:

\begin{equation}
    \frac{d^2 \vec r}{dt} = -\frac{1}{r^2} \hat r
\end{equation}

If we start with a particle orbiting at $|r|=1$ with $|v|=1$ and $\vec r \cdot
\vec v = 0$, the particle should stay on an orbit with $r=1$ forever.  Let's see
what happens with the best and worst case integrators we've seen so far.

\begin{demo}[orbits.cpp]
    Show that both Euler's method and RK4 do very bad at keeping a planet on
    its orbit.
\end{demo}

Are we just screwed?  Is there any way to fix this problem?  To find out, we
should first try and understand the source of the problem.  We might think that
it's just the truncation error, but if that was the case than why is it
systematic? Let's see if we can find the source for the simpler case, our
Euler's method.

Recall Hamilton's equations:

\begin{equation}
    \frac{dq}{dt} = \frac{\partial H}{\partial p} \qquad \frac{dp}{dt} =
    -\frac{\partial H}{\partial q}
\end{equation}

So for the SHO, our equations of motion become the familiar ones we've seen
since Physics I:
\begin{equation}
    q' = p \qquad p' = - q
\end{equation}

So our Euler update step for $p$ and $q$ will be: 

\begin{equation}
    \begin{aligned}
        p_{n+1} & = p_n - \Delta t q_n \\
        q_{n+1} & = q_n + \Delta t p_n \\
    \end{aligned}
\end{equation}

Let's see what happens when we calculate $H_{n+1}$:
\begin{equation}
    \begin{aligned}
        H_{n+1} & = \frac{1}{2}(p_{n+1}^2 + q_{n+1}^2) =
        \frac{1}{2}\left[\left(p_n -
        \Delta t q_n\right)^2 + \left(q_n + \Delta t p_n\right)^2 \right]\\
        &= \frac{1}{2}\left[(p_n^2 -2\Delta t p_n q_n + \Delta t^2 q_n^2) +
        (q_n^2 + 2\Delta p_n q_n + \Delta t^2 p_n^2) \right] \\
        &= \frac{1}{2}(1+\Delta t^2)(p_n^2 + q_n^2) \neq H_n
    \end{aligned}
\end{equation}

Well there we have it, the energy's not conserved because our update scheme
explicitly does not conserve it!  The energy errors go down as we decrease the
timestep, but they are monotonically increasing.  Euler's method, for a SHO,
will always have the energy increase.  If we did the same excercise with RK4 (we
can, but it's a bit tedious), we would also find that $H_{n+1} \neq H_n$.

So, we know that RK4 and Euler don't work.  Can we figure out a way to build a
method that does work?  Absolutely!  It turns out the simplest example we can
come up with is almost identical to Euler's method.  It looks like this:

\begin{equation}
    \begin{aligned}
        p_{n+1} & = p_n - \Delta t q_n \\
        q_{n+1} & = q_n + \Delta t p_{n+1} \\
    \end{aligned}
\end{equation}

You might think that this is an implicit method, but it's not quite, since we
calculate $p_{n+1}$ using only information from the $t_n$ step:  

\begin{equation}
    \begin{aligned}
        p_{n+1} & = p_n - \Delta t q_n \\
        q_{n+1} & = q_n + \Delta t p_n - \Delta t^2 q_n = \Delta t p_n +
        (1-\Delta t^2) q_n\\
    \end{aligned}
\end{equation}

This method is called the Euler-Cromer or Semi-Implicit Euler Method. Let's
check if this is indeed energy conserving:

\begin{equation}
    \begin{aligned}
        H_{n+1} & = \frac{1}{2}(p_{n+1}^2 + q_{n+1}^2) =
        \frac{1}{2}\left[(p_n - \Delta t q_n)^2 + (\Delta t p_n + (1-\Delta
        t^2)q_n)^2\right]\\
        &= \frac{1}{2}\left[(p_n^2 - 2\Delta t p_n q_n + \Delta t^2 q_n^2) +
        (\Delta t^2 p_n^2 + 2\Delta t(1-\Delta t^2)p_nq_n + (1-\Delta t^2)^2
        q_n^2)\right]\\
        &= \frac{1}{2}\left[p_n^2 + q_n^2 + \Delta t^2(p_n^2-q_n^2) + \Delta t^4
        - 2\Delta t^3 p_n q_n\right]
    \end{aligned}
\end{equation}

Well what the heck?  Why did I just waste my time writing that if it doesn't
work?  Did I make a mistake?  Let's check it empirically.

\begin{demo}[ec\_energy.cpp]
   Show that the energy error is reasonably big, but it doesn't grow over time. 
\end{demo}

Well well well, it looks like the energy isn't \textit{conserved} per-se, but
the errors are bounded: the energy neither grows nor shrinks.  It turns out the
thing we're looking for is not that the \textit{true} Hamiltonian is
unchanged step-to-step, but that a \textit{numerical} Hamiltonian is conserved.
This numerical Hamiltonian is essentially just:

\begin{equation}
    H_{numerical} = H_{true} + H_{error}
\end{equation}

As long as that is conserved, the error will always be bounded.  A more formal
way of looking at this is using Liouville's Theorem, which states that energy
conserving systems have constant phase-space volume.  Therefore, a
transformation matrix $\mathbf{M}$:

\begin{equation}
    \begin{pmatrix}
        q_{n+1} \\
        p_{n+1} 
    \end{pmatrix} = \mathbf{M}
    \begin{pmatrix}
        q_{n} \\
        p_{n} 
    \end{pmatrix} 
\end{equation}

will conserve energy (within the bounds of some error) if and only if
$\mathbf{M}$ is \textit{symplectic}, or if 
\begin{equation}
    \det\mathbf{M} = 1.  
\end{equation}

We could dive very deep into this if this course was a course on differential
geometry or a graduate mechanics class, but for now we'll leave the definition
there.  If we look at the two transformation matrices for Euler and
Euler-Cromer, we have:

\begin{equation}
    \det \mathbf{M}_{euler} = 
    \begin{pmatrix}
        1 & \Delta t \\
        -\Delta t & 1 \\
    \end{pmatrix} = 1 + \Delta t^2
\end{equation}

Menwhile,
\begin{equation}
    \det \mathbf{M}_{euler-cromer} = 
    \begin{pmatrix}
        1 - \Delta t^2 & \Delta t \\
        -\Delta t & 1 \\
    \end{pmatrix} = 1 
\end{equation}

This means that phase-space volume is conserved, and therefore the numerical
Hamiltonian is conserved.  It turns out for the Euler-Cromer method, the
numerical Hamiltonian is:
\begin{equation}
    H_{numerical} = \frac{1}{2}(p^2 + q^2) - \frac{\Delta t}{2}pq
\end{equation}

If we look again at the energy error, but this time look at $H_{numerical}$
instead of $H_{true}$, we can see this is perfectly conserved.  Deriving this
generally is somewhat involved, and not always possible.

\begin{demo}[ec\_energy.cpp]
   Change what's printed out to show the numerical Hamiltonian instead.
\end{demo}

Euler-Cromer works as a symplectic integrator, but it's only first.
There's a second-order method that's commonly used that has a second advantage
beyond just being higher order.  This method, called by many names
(\textit{leapfrog, Verlet, Kick-Drift-Kick}) is not only symplectic, it has
perfect time-reversal symmetry.  This helps it further beat down the errors in
energy, beyond just having the error not grow.  Leapfrog is basically the
standard time integrator for most astrophysical problems that don't involve
planets, as well as molecular dynamics simulations of individual atoms/molecules
interacting.

The name Kick-Drift-Kick is actually a pretty good explanation of Leapfrog.  We
first ``kick'' the thing, updating it's velocity.  We then let it drift
(updating it's position), and finally give it another kick.  The two kicks are
done on half-steps, and the drift on a full timestep.  The algorithm looks like
this (not just for the SHO, but in general):

\begin{equation}
    \begin{aligned}
        v_{n+1/2} &= v_n + \frac{\Delta t}{2} a(x_n) \\
        x_{n+1} &= x_n + \Delta t v_{n+1/2}
        v_{n+1/2} &= v_{n+1/2} + \frac{\Delta t}{2} a(x_{n+1}) \\
    \end{aligned}
\end{equation}

This method gives much smaller errors (it is second order after all!).
\begin{demo}[kdk\_energy.cpp]
    Show that the energy error is smaller than with Euler-Cromer.
\end{demo}

We can also run this in reverse to perfectly reproduce our initial conditions!
\begin{demo}[time\_reverse.cpp]
    Show that time reversal works with Leapfrog.
\end{demo}

Energy conservation isn't something you need to worry about for every ODE you
might integrate.  For many ODEs, there is no concept of energy. There are plenty
of ODEs you might encounter that aren't describing physics problems.  ODEs
describing traffic flow, the populations of different species, or the prices of
a given commodity all can be solved numerically, but have no concept of energy
in the system. Some physics problems also aren't derived from Hamilton's
equations, and so don't really care about energy conservation.  Even in cases
where we \textit{do} care about energy conservation, a high-order method like
RK4 might not be so bad as long as the dynamical time is long compared to the
timescales we integrate over.  Choosing the correct method is in part a question
of knowing \textit{what about the system you actually care about}.  There is no
silver bullet in numerical integration.

\subsection{Boundary Value Problems}
From time to time, we might find ourselves faced with a different kind of
differential equation problem (har har).  Rather than solving an ODE with the
values specified at some point and evolving it forward in time, we may instead
have an ODE with specific values given at certain points.  This problem might
look a bit like this: 

\begin{equation}
    \frac{d^2 u}{d x^2} = f\left(u, \frac{du}{dx}\right)
\end{equation}

To solve this, we have conditions that must be satisfied called \textit{Boundary
conditions}.  These are typically given at $x=0$ and $x=1$, since we can always
scale the problem to fit these limits (think of changing a unit system, for
example).

Unlike with an IVP, where we need the value of $u$ and $u'$ at $t=0$, there are
actually many different boundaries for a BVP.  For both the left side and the
right side, we can potentially have either $u$, $u'$, or both specified.  These
are often referred to in the literature as \textit{Dirichlet} boundary
conditions when we have $u(0)$ and $u(1)$, \textit{Neumann} boundary conditions when we
have $u'(0)$ and $u'(1)$, and \textit{Cauchy} when both are specified.  As you might
imagine, if we have a mix, this is referred to as a mixed boundary condition.
We may also have an eigenvalue up in the mix, which can select from many
possible solutions.  We'll keep things simple and focus on just plain BVPs,
since the methods for solving them are essentially the same as for eigenvalue
problems.

Let's consider a slightly different problem than the usual harmonic oscillator
we've seen before.  The regular SHO is a homogeneous ODE, and so it has a
trivial solution where $x=0$ everywhere.  Let's play with this equation instead:

\begin{equation}
    \frac{d^2 u}{dx^2} = -\frac{\pi^2}{4}(u + 1)
\end{equation}

This thing also has analytic solutions, which look like:
\begin{equation}
    u(x) = a\sin\left(\frac{\pi x}{2}\right) + b\cos\left(\frac{\pi x}{2}\right)
    -1
\end{equation}

where $a$ and $b$ are constants that depend on the boundary value.  Let's see if
we can figure out how to solve this thing.  If we were solving an initial value
problem, we would probably want to split this second-order ODE into two coupled
first-order ODEs:
\begin{equation}
    \begin{aligned}
        \frac{dy}{dx} &= z \\
        \frac{dz}{dx} &= -\frac{\pi^2}{4}(y+1) \\
    \end{aligned}
\end{equation}

If this was an IVP, all I would need would be the values of $y(0)$ and $z(0)$,
and I could integrate forward.  But what if, instead, I have $y(0)$ and $y(1)$
(Dirichlet boundary conditions).  I don't know the initial value of $z(0)$, so I
can't just integrate forward like I normally would for an IVP.  Or can I?

Let's try and solve for these boundary conditions: $u(0) = 0$ and $u(1) = 1$.
If I had some way of guessing the correct value of $z(0)$ that would give
$u(1)=1$, I would be golden.  Maybe guessing is in fact a way forward?  Let's
see how far that can get us.

\begin{demo}[boundary.cpp]
    Try different guesses for $z_0$ to see how the solutions behave.  Let
    everyone try different numbers until we converge towards a solution.
\end{demo}

What does that exercise seem like to you?  If you thought root finding, you'd be
right.  What we were trying to do was find some value of $z(0)= z_0$ such that when
we solved the IVP with $y(0)=0$ and $z(0)=z_0$, the value of $y(1)=1$ when the
integration reaches $x=1$.  Remember that root-finding is the process of finding
when some function $f(\alpha) = 0$.  In our case, we have something that looks
like this:

\begin{equation}
    f(\alpha) = \mathrm{solve\_ivp}(y_0=0,z_0=\alpha) - y(1)
\end{equation}

All we need to do then is feed this into our root finder, and away we go.  So,
let's combine the bisection method for root finding with the 4th-order RK
method.  This process of guessing a set of initial conditions, solving an IVP
with those ICs, and then root-finding the error relative to the second boundary
condition is called the \textit{shooting method}.  It's like we're shooting
artillery at a target.  The first few shots miss, but each time we dial in the
elevation and/or the powder charge to eventually land right on target (our
second boundary).

\begin{demo}[shooting\_bisect.cpp]
    Outline the code for using bisection to solve the problem.  Check that it
    works, and that it's more expensive the smaller the error tolerance is.
\end{demo}

We know that bisection's fast, but it's not the fastest to converge.  Since we
don't have an analytic function for:
\begin{equation}
    \frac{d}{d\alpha}f(\alpha) = \frac{d}{d\alpha} \mathrm{solve\_ivp}(y_0=0,z_0=\alpha) 
\end{equation}

We probably can't throw Newton's method at this.  We can, however, try using the
secant method, where we estimate that derivative as:
\begin{equation}
    \frac{d}{d\alpha}f(\alpha) = f(\alpha_{1})\frac{\alpha_{1} -
    \alpha_{0}}{f(\alpha_1)-f(\alpha_0)}
\end{equation}

Let's see how much faster this is:
\begin{demo}[shooting.cpp]
    Show that this method converges in 1 iteration.  Change the bounds and show
    that the number of iterations doesn't change.  Then do the same thing for
    the tolerance.
\end{demo}

How can this thing always be converging in 1 iteration?  That seems too good to
be true?  Well, the shooting method with secant root finding will
\textit{always} converge to the exact solution (within machine precision) in a
single iteration when one condition: that the ODE is a linear one.  Linear ODEs
look like this:

\begin{equation}
    u'' + a(x)u' + b(x)u = c(x)
\end{equation}

A huge swath of ODEs that you might encounter are linear, and they all have the
nice property of superposition: any solution $u(x)$ can be composed by the
linear combination of:

\begin{equation}
    u(x) = Au_{\alpha_0}(x) + Bu_{\alpha_1}(x)
\end{equation}

With the secant method, we've exactly determined the values of $A$ and $B$ for
our two initial guesses that will give $u(1) = 1$!

\section{Monte Carlo Methods}
All of the techniques we've seen so far are designed for human use.  Newton's
method for root finding was first published (in some form) in 1669.  Simpson's
rule was actually derived before Thomas Simpson was even born, by Johannes
Kepler in 1615.  Euler's method was published first in 1768.  The Monte Carlo
method is the first numerical method we're going to study that was created with
mechanical calculation in mind.  

In 1945, the first general-purpose (Turing complete) electronic, digital
computer ever built.  It was originally intended for calculating artillery
firing solutions (similar to the problem you solved in Homework 9).  This
machine, ENIAC, ended up primarily being used to calculate reaction rates and
scattering processes in the (then hypothetical) thermonuclear bomb.  In 1946,
Stanislaw Ulam (who has the dubious distinction, along with the strange little
man Edward Teller, of being the father of the Hydrogen Bomb) proposed using
random numbers to simulate random-ish processes, like the scattering of neutrons
passing through a solid.  2 years later, John von Neumann and Nicholas
Metropolis programmed ENIAC to use this method to simulate said hydrogen bomb.
It's name, by the suggestion of Metropolis, was the \textit{Monte
Carlo method}, which was mostly a dunk on Ulam, who had a gambling-addicted
uncle who borrowed his money because he ``just had to go to Monte Carlo''
(that's a direct quote from an article Metropolis wrote on it).

Monte Carlo methods are powerful in part because they are relatively stupid.
They rely on the brute force that computers provide to smash their way towards a
solution without too much cleverness (though a little bit of cleverness can make
them \textit{much} more effective).  To use them, we first need to learn how to
work with random numbers in C++.

\begin{demo}[random.cpp]
    Do a live-coding demo of how we work with random numbers.
\end{demo}

The simplest example of how we can use randomness to solve problems is with
Monte Carlo integration to solve for the value of $\pi$.  If we enclose a
quadrant of a circle with radius $L$ in a square with side length $L$, the area
of the circle $A_C$ and the square $A_S$ will be:

\begin{equation}
    A_C = \frac{\pi}{4}L^2 \qquad A_S = L^2
\end{equation}

respectively.  Therefore, the ratio of these two values is simply $\pi/4$.  If
we draw two random numbers $x,y$ each between 0 and $L$, the probability that
this random point will fall within the circle will be:

\begin{equation}
    P[\sqrt{x^2+y^2} <= L] = \frac{A_C}{A_S} = \frac{\pi}{4}
\end{equation}

Therefore, the expectation value for the fraction of points within this quadrant
should be $\pi/4$.  Let's see if this works!

\begin{demo}[pi.cpp]
    Do a live-coding demo of how we can find $\pi$ with a Monte-Carlo Method.
\end{demo}

It turns out that we can integrate \textit{any} function simply using this Monte
Carlo method:

\begin{equation}
    I = \int_a^b f(x) dx \approx \frac{b-a}{N} \sum_{i=0}^N f(x_i)
\end{equation}

\begin{demo}[mc\_int.cpp]
    Show how we can integrate any arbitrary function with MC integration.
\end{demo}

You can see though that the accuracy here isn't very good. Let's see how it
converges...

\begin{demo}[mc\_converge.cpp]
    Show how we can integrate any arbitrary function with MC integration.
\end{demo}

You can see here that the convergence actually isn't very good!  In fact, the
convergence here is a purely statistical one: we no longer have a truncation
error, but instead are dealing with how likely it is that we've reasonably
sampled our function.  This means that the convergence generally will scale as
$\mathcal{O}(N^{1/2})$, worse than even the lowest-order quadrature rule.

This situation seems to suggest that Monte Carlo integration is nothing more
than a fun trick, but not a practical way of integrating a function.  However,
there is one class of problems where Monte Carlo integration is the
\textit{only} tractable way of solving the integral:  when the dimensionality of
the problem becomes large.  

\begin{demo}[dimensionality.cpp]
    Demonstrate the ``curse of dimensionality''
\end{demo}

When we have a high-dimensional integral, as we often do in statistical and
quantum mechanics, a quadrature method that converges as $\mathcal{O}(N^a)$ in
one dimension will instead converge as $\mathcal{O}(N^{a/D})$, where $D$ is the
dimensionality of the problem.  As long as $a/D<<1/2$, then Monte Carlo
integration will vastly outperform numerical quadrature.

\begin{demo}[converge.cpp]
    Show how the MC method outperforms quadrature with high dimensionality.
\end{demo}

\subsection{Markov Chains and the Metropolis Algorithm}
We can easily draw random numbers from a uniform distribution using the built-in
\texttt{rand()} function in C/C++, but what if we want random numbers from a
different distribution?  Worse yet, what if we don't fully know that
distribution, but instead only know that probabilities from it are proportional
to some function?

\begin{equation}
    p(\vec x) \propto \pi(\vec x)
\end{equation}

To actually be a PDF, $\pi(\vec x)$ needs to be normalized:

\begin{equation}
    p(\vec x) = \frac{\pi(\vec x)}{\int_D \pi(\vec x) d\vec x}
\end{equation}

If the domain $D$ has high dimensionality, this will become a nightmare to
integrate with quadrature, as we demonstrated before.  But at the same time,
Monte-Carlo integration of it will still converge slowly.  Perhapse there is a
way to draw random numbers from this distribution \textit{without} knowing the
normalizing constant?

It turns out there indeed is!  The way is to use a \textit{Markov Chain}.  A
Markov chain is a series of ``states'', with transitions between them governed
by some probability.  We can construct one using the \textit{Metropolis
algorithm}, and when we're done, we will have an algorithm for generating
numbers from any arbitrary distribution with $p(x)\propto\pi(x)$.  

The basic idea is heavily inspired by statistical mechanics, and it looks like
this: if $p(x)\propto\pi(x)$, then a system in equilibrium will have no net
``flux'' of states:
\begin{equation}
    \begin{aligned}
        p(x_0)p_{trans}(x_0 \rightarrow x_1) &= p(x_1)p_{trans}(x_1 \rightarrow
        x_0) \\
        \pi(x_0)p_{trans}(x_0 \rightarrow x_1) &= \pi(x_1)p_{trans}(x_1 \rightarrow
        x_0) \\
    \end{aligned}
\end{equation}

This means that the probability of the system jumping from state $x_0$ to $x_1$
doesn't depend on the normalizing constant!  It is just given as:
\begin{equation}
    p_{trans}(x_0 \rightarrow x_1) = \frac{\pi(x_1)}{\pi(x_0)}
\end{equation}

This lets us build a beautiful, simple algorithm, the \textit{Metropolis}
algorithm, for drawing random numbers from some distribution:

\begin{enumerate}
    \item Pick some state $\vec x_0$
    \item Draw a random vector $\vec \alpha$ with each element uniformly
        distributed between $[0,1]$, and generate a new trial state,
        $\vec x_{trial} = \vec x_i + h(2\vec\alpha -1)$
    \item Calculate $R=\pi(x_{trial})/\pi(x_i)$
    \item Draw a new random number, $\beta$, between $[0,1]$.  If $R>\beta$,
        accept the trial, setting $x_{i+1} = x_{trial}$.  Otherwise, reject it
        and set $x_{i+1} = x_i$
\end{enumerate}

That's it! The set of all the values in this chain will follow the true PDF
$p(x)$ \textit{even though we don't know it's normalization!} Let's see it in
action.  We'll evaluate a PDF that's the sum of three Gaussians:

\begin{equation}
    \pi(x) = 6\exp\left(-4(x+4)^2\right) + 2\exp\left(-0.2(x+1)^2\right) + \exp\left(-2(x-5)^2\right)
\end{equation}

\begin{demo}[metropolis.cpp]
    Live-code the metropolis algorithm, show it works.
\end{demo}

One trick we can use with this is to introduce \textit{importance sampling}.
This can significantly improve the accuracy of a Monte-Carlo integration.
Let's say we have a function that grows steeply, say for example:

\begin{equation}
    f(x) = x^{20}
\end{equation}

The integral of this will of course be:
\begin{equation}
    I = \int_a^b f(x) dx = \frac{b^{21}}{21} - \frac{a^{21}}{21} \approx
    \frac{b^{21}}{21}
\end{equation}

If $b>a$, since the exponent is so large.  This means the integral depends
almost exclusively on points near $b$, not $a$.  So what we can do to improve
our integral is to draw points \textit{near} $b$.  This is equivalent to
transforming our integral into something nearly constant.  If we pick a function
$p(x)$ such that:

\begin{equation}
    \int_a^b p(x)dx  = 1
\end{equation}

and

\begin{equation}
    p(x)f(x) \approx const
\end{equation}

Then our standard Monte-Carlo integration can be replaced with:

\begin{equation}
    I = \int_a^b f(x) dx = \int_a^b \frac{f(x)}{p(x)} (p(x) dx) \approx
    \frac{1}{N} \sum_i^{N} \frac{f(x_i)}{p(x_i)}
\end{equation}

Where each $x_i$ is drawn from $p(x)$.  This method is called \textit{importance
sampling}, and it is a powerful way to improve the convergence of Monte Carlo
Integration.

\begin{demo}[importance.cpp]
    Show how importance sampling works.
\end{demo}

\section{Partial Differential Equations}
We've now seen methods for estimating derivatives and integrals, finding roots,
and solving ODEs.  We now turn to the most interesting, and often the most
challenging, computational problem class in numerical physics: \textit{Partial
Differential Equations}.  There is a nearly infinite variety of potential PDEs
that can be studied, but in the context of physics, three broad classes show up
again and again: \textit{Hyperbolic}, \textit{Parabolic}, and \textit{Elliptic}
equations.  Why they are called this is something for a math course, but we can
understand the key distinctions that we would care about by looking at some key
examples.

Each of these classes of PDEs each have examples that you are likely
already familiar with.  The canonical hyperbolic PDE is the wave equation:

\begin{equation}
    \frac{\partial^2 \phi}{\partial t^2} = c^2 \nabla^2 \phi
\end{equation}

The key feature of hyperbolic PDEs is that the solution propagates at a finite,
real speed (in the case of the wave equation, that speed is $c$).  A
perturbation made at some point will only change the value of $\phi$ within a
radius of $ct$: everywhere else the values are unchanged. The canonical
example of a parabolic PDE is the heat equation:

\begin{equation}
    \frac{\partial \phi}{\partial t} = k \nabla^2 \phi
\end{equation}

Unlike with hyperbolic PDEs, parabolic systems transmit information
instantaneously: a perturbation made at some point will instantaneously change
the value of $\phi$ everywhere.  Naturally, this is unphysical, but if the speed
of a signal propagating is sufficiently fast, we can treat it as effectively
infinite.

The final class is the elliptical PDE, and the canonical example here is
Poisson's equation:

\begin{equation}
    \nabla^2 \phi = f
\end{equation}

Notice that this has no time dependence: the values of $\phi$ are set purely by
the boundary conditions and the source function $f$.  This too implies an
infinite speed at which information is propagated.

\subsection{Hyperbolic PDE: The Linear Advection Equation}
The simplest example of a hyperbolic PDE, and one that is very easy to reason
about, is the linear advection equation:

\begin{equation}
    \frac{\partial a}{\partial t} = u\frac{\partial a}{\partial x}
\end{equation}

\end{document}
