\documentclass{article}
\usepackage{geometry}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts,amsthm}

\theoremstyle{demo}
\newtheorem{demo}{Demo}[section]

\begin{document}
\title{Physics 4420: Computational Skills in Physics}
\maketitle
\section{Numerical Calculus}
\subsection{Differentiation}
Think back to your first course in calculus.  After finishing with limits, you
were likely finally presented with the definition of the derivative:
\begin{equation}
    \frac{df}{dx}(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}
\end{equation}

Pretty much the entire basis of numerical (computational) physics, at least when
we're talking about calculus, is taking that definition and fudging
$h\rightarrow 0$ to $h\rightarrow\textrm{small}$.  We're just going to fudge in
a careful, controlled manner :).

Let's see how this actually works, for a function we know the derivative of
(let's say sine).

\begin{demo}[diff.cpp]
    Show that the results look OK for $h=0.1$ using \textit{plotxy} and showing
    the plain numbers.  Let the class try the same thing with a function of
    their choice.
\end{demo}

That looks \textit{ok}, but not great.  You can see that the values aren't
exactly identical.  That's not surprising though, since we're \textit{supposed}
to be taking $h\rightarrow0$.  Let's see what happens when we make it closer to
zero.

\begin{demo}[diff\_converge.cpp]
    Show how the average error changes as we decrease $h$. No need to plot,
    numbers are enough.  Have the class try the same thing for their function.
\end{demo}

You can see here that the error goes down roughly linearly: halve $h$, and halve
the error.  The rate at which the error of a scheme goes down relative to the
resolution or step size is called it's \textit{convergence rate}.
But can we do better?

\subsubsection{Central Difference}
To do better, let's use a tool that, if it isn't already your best friend, will
soon become it: the Taylor Series Expansion.  Recall this is defined as:

\begin{equation}
    \begin{aligned}
        f(x) & = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2}(x-x_0)^2 + ... \\
        & = \sum_{n=0}^\infty \frac{1}{n!} \frac{d^n f}{dx^n}(x_0) (x-x_0)^n
    \end{aligned}
\end{equation}

Let's clean this up a bit to match the kind of notation that's common in
computational physics.  Let's define:

\begin{equation}
    h = x - x_0 \equiv \Delta x
\end{equation}

$\Delta x$ will be our ``step size'', a term we will become intimately familiar
with as we progress.  We can then define points recursively, where:

\begin{equation}
    x_{i+1} = x_i + \Delta x
\end{equation}

and $f(x_i) = f_i$. In this new notation, our Taylor series expansion looks
like:
\begin{equation}
    f_{i+1} = f_i + f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 + ... =
    \sum_{n=0}^\infty \frac{1}{n!} \left.\frac{d^n f}{dx^n}\right|_i \Delta x^n\\
\end{equation}

With this, we can re-arrange to get an expression not for $f_i$, but instead
$f'_i$:

\begin{equation}
    \begin{aligned}
        f'_i & = \frac{f_{i+1} - f_i}{\Delta x} - \frac{f''_i}{2}\Delta x - ...\\
    \end{aligned}
\end{equation}

Notice that the first term here is what we plugged in to our \textit{finite
difference} scheme.  That's the formal name for this method of estimating a
derivative, since what we've done is taken an exact expression for the
derivative (our Taylor series), and truncated to include a finite number of
terms.  We can also see here why the error grows linearly:  the first omitted
term is linear in $\Delta x$.

Here's a good point to introduce ``Big-O Notation''.  We're going to frequently
find ourselves facing infinite sums that depend on different powers of something
(frequently our step size $\Delta x$).  It's useful to be able to talk about
these sums just in terms of their largest term.  If we've got a parameter like
$\Delta x$ that is small, then the highest power will be the largest.  So if
that term is proportional to $\Delta x$, then the omitted term will be
$\mathcal{O}(\Delta x)$.  Note that this term is always positive, which makes
sense if you think about it: we don't know what the sign of some arbitrary
high-order derivative is, and if you think about this as an error term the
errors are usually symmetric about zero.

Thus, our finite difference can be expressed as:

\begin{equation}
    f'_i = \frac{f_{i+1} - f_i}{\Delta x} + \mathcal{O}(\Delta x)
\end{equation}

That first term is our error, and it tells us it scales linearly as $\Delta x$.

It turns out we can do better than an error that's linear in $\Delta x$ if we
are clever.  Let's look at the Taylor expansion for $f_{i+1}$ and $f_{i-1}$
(remember this would be $f(x+\Delta x)$ and $f(x-\Delta x)$):

For $f_{i+1}$, we get:
\begin{equation}
    f_{i+1} = f_i + f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 +
    \mathcal{O}(\Delta x^3)
    \label{fipone}
\end{equation}

Meanwhile, for $f_{i-1}$, we get:
\begin{equation}
    f_{i-1} = f_i - f'_i \Delta x + \frac{f''_i}{2}\Delta x^2 +
    \mathcal{O}(\Delta x^3)
    \label{fimone}
\end{equation}

Because in this case we have $x-x_0 = -\Delta x$.  If you've had enough coffee
today, you might notice something powerful about this:  the term with $f'_i$ has
the opposite sign to the first omitted term.  So, if we subtract one term from
the other, we get:
\begin{equation}
    f_{i+1} - f_{i-1} = 2\Delta x f'_i + \mathcal{O}(\Delta x^3)
\end{equation}

Re-arranging this to solve for $f'_i$ gives us:
\begin{equation}
    f'_i = \frac{f_{i+1} - f_{i-1}}{2\Delta x}  + \mathcal{O}(\Delta x^2)
    \label{central_diff}
\end{equation}

Check that out! The error term here grows \textit{quadratically} with $\Delta
x$.  This scheme is called a \textit{central difference}.  If you think about
what this scheme is doing, it is estimating the derivative at point $x_i$ by
using information at $x_{i+1}$ and $x_{i-1}$, while \textit{ignoring} the value
at $x_i$!  It's shocking that this works!  Or does it?  Let's try it out.

\begin{demo}[diff\_central.cpp]
    Show that the central scheme has errors that scale with $\Delta x^2$.  Show
    that it converges so quickly you run out of float precision, and you need
    doubles to see further convergence.  Have the class try it on their
    function.  
\end{demo}

\subsubsection{Five-point Scheme and Stencils}
We can go even higher in accuracy by using this approach to cancel the
$\mathcal{O}(\Delta x^2)$ term.  Let's fill in that term in our expression for
$f_{i+1} - f_{i-1}$:

\begin{equation}
    f_{i+1} - f_{i-1} = 2f'_i \Delta x + 2 \frac{f^{(3)}}{6} \Delta x^3 +
    \mathcal{O}(\Delta x^5) = 2f'_i \Delta x + \frac{f^{(3)}}{3} \Delta x^3 +
    \mathcal{O}(\Delta x^5)
\end{equation}

The last term here is $\mathcal{O}(\Delta x^5)$ because of the same cancellation
that got rid of our $\mathcal{O}(\Delta x^2)$ term: any even power of $\Delta x$
will be cancelled!  So what we need now is some way to ditch the $\Delta x^3$
term.  Perhaps the same approach as before could be useful, but this time let's
look at $f_{i+2}$ and $f_{i-2}$.

\begin{equation}
    f_{i+2} = f_i + f'_i (2 \Delta x) + \frac{f''_i}{2}(2 \Delta x)^2 +
    \frac{f^{(3)}}{6} (2 \Delta x)^3 + \mathcal{O}(\Delta x^5)
\end{equation}

\begin{equation}
    f_{i-2} = f_i - f'_i (2 \Delta x) + \frac{f''_i}{2}(2 \Delta x)^2 -
    \frac{f^{(3)}}{6} (2 \Delta x)^3 + \mathcal{O}(\Delta x^5)
\end{equation}


This makes sense, since $f_{i+2}$ just means we're jumping forward from $x_i$ by
$2\Delta x$.  If we subtract these from each other, we get:

\begin{equation}
    f_{i+2} - f_{i-2} = 4f'_i \Delta x + \frac{8f^{(3)}}{3} \Delta x^3 +
    \mathcal{O}(\Delta x^5)
\end{equation}

You can probably see exactly what we need to do:  $8(f_{i+1}-f_{i-1}) -
(f_{i+2}-f_{i-2})$ will cancel the $\Delta x^3$ term.  If we plug that in, and
do some re-arranging, we will get what's called the \textit{five-point scheme}:

\begin{equation}
    f'_i = \frac{1}{12\Delta x}(f_{i-2} - 8f_{i-1} + 8f_{i+1} - f_{i+2}) +
    \mathcal{O}(\Delta x^4)
\end{equation}

\begin{demo}[diff\_fivepoint.cpp]
    Show that the five-point scheme has errors that scale with $\Delta x^4$.  
\end{demo}

This probably has you wondering: why can't we just repeat this exercise
ad-nauseum until we have a scheme that has errors always well below
floating-point roundoff?  Well, there are two downsides to using higher-order 
finite difference schemes.  The first is fairly simple: the five-point formula
requires more operations.  For a central difference, we have 2 function calls,
but the five-point requires 4.  If those are expensive, it might be cheaper to
use the central scheme.  The second issue is related to where the function is
being evaluated.  

The five-point scheme is sometimes referred to as the five-point
\textit{stencil}, because it uses five discrete positions: $x_{i-2}$ through
$x_{i+1}$.  This means we're using four neighbouring points to calculate our
derivative.  If we want to shrink our error term even more, this stencil will
need to grow.  Eventually, we'll be covering a huge fraction of the function's
domain just to calculate a derivative.  This will be very important when we
start using these tools to solve differential equations.

\subsubsection{Higher Order Derivatives and Catastrophic Cancellation}
We can use a similar approach to calculate higher-order derivatives as well.  We
probably don't want our high-order derivatives to depend on lower-order
derivatives, so let's construct a second derivative scheme for $f''_i$ that
depends only on $f$.  If we go back to equation~\ref{fipone} and
equation~\ref{fimone}, we can see how to remove the first derivative: just add
'em!

\begin{equation}
    f_{i+1} + f_{i-1} = 2f_i + f''_i \Delta x^2 + \mathcal{O}(\Delta x^3)
\end{equation}

So our second-derivative scheme will be:

\begin{equation}
    f''_i = \frac{f_{i-1} - 2f_{i} + f_{i+1}}{\Delta x^2} + \mathcal{O}(\Delta
    x)
\end{equation}

Let's see if it works.

\begin{demo}[second\_diff.cpp]
    Show that the results look OK for $h=0.1$ using \textit{plotxy} and showing
    the plain numbers.  Let the class try the same thing with a function of
    their choice.
\end{demo}

This scheme should also give us linear convergence as $\Delta x$ shrinks. Let's
test that just to be sure.
\begin{demo}[second\_convergence.cpp]
    Show that the results \textit{sort of} converge, but not very well.  Try it
    again with doubles.  Why does it not work?
\end{demo}

What the heck is going on here?  It turns out we have been playing somewhat
fast-and-loose: we've assumed that real numbers and floats are the same thing.
But they aren't.  What we've seen here is a case of \textit{catastrophic
cancellation}.  Recall that the way floats are stored gives us $\sim7$ digits of
precision for single-precision (32 bit) and $\sim16$ digits of precision for
doubles (64 bit).  To calculate the second derivative, we've added together two
numbers, and subtracted a third as part of our calculation:
\begin{equation}
    f_{i-1} - 2f_{i} + f_{i+1}
\end{equation}

Let's look closely at what happens when we subtract two numbers that have errors
associated with them (in our case, this is due to the finite precision that is
unavoidable with storing real numbers as finite, floating-point numbers).  If we
have two numbers, $\tilde\alpha$ and $\tilde\beta$, each with errors relative to their
true values $\alpha$ and $\beta$ that are
$\tilde\alpha = \alpha + \mathcal{O}(\epsilon \alpha)$ and  $\tilde\beta = \beta
+ \mathcal{O}(\epsilon \beta)$ respectively (where $\epsilon$ is some small
number), then

\begin{equation}
    \tilde \alpha - \tilde \beta = \alpha + \mathcal{O}(\epsilon \alpha) - \beta +
    \mathcal{O}(\epsilon \beta) 
\end{equation}

What happens if the true value of $\alpha$ and $\beta$ are close?  In the most
extreme case, where $\alpha = \beta$, the fractional difference between the true
value and the calculated value will become
\begin{equation}
    \frac{\tilde \alpha - \tilde \beta}{\alpha - \beta}  =
    \frac{\mathcal{O}(\epsilon \alpha)}{0} = \infty
\end{equation}

What this warns us about is that when we try to subtract floating point numbers,
if the difference between those numbers is small compared to the overall
precision, we may end up growing the error in the floating point representation
well beyond what we might expect.

\begin{demo}[cancel.cpp]
    Show how catastrophic cancellation can give a result with errors much larger
    than 1 part in $10^7$.
\end{demo}

\subsection{Integration}
Calculating the integral of a function numerically is similar to how we started
calculating numerical derivatives.  We'll start with the way you were likely
first introduced to the derivative, as a Riemann sum:

\begin{equation}
    \int_a^b f(x) dx = \lim_{N \to \infty} \sum_{i=1}^N f\left(a +
    \frac{i(b-a)}{N}\right) \frac{b-a}{N}
\end{equation}

If we once again don't let $N\rightarrow\infty$, but instead just use a ``big''
value for N, we can approximate our integral as:
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N f_i \Delta x
\end{equation}

Where $\Delta x = (b-a)/N$.  Let's see how this works in practice.

\begin{demo}[int.cpp]
    Show how well the plain old rectangle rule works.  Notice that the error
    looks like it gets worse as $x$ increases.  Why is that?  Get the class to
    try their own function.
\end{demo}

Just like with numerical differentiation, we can be more clever about how we do
this in order to reduce the error and improve the convergence.  You've probably
already seen some of these in your Calculus classes in the past.  These methods
are called the Newton-Cotes quadrature rules. We can first try to improve the
estimation of the each component by using the trapezoid Rule:
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N (f_i + f_{i+1}) \frac{\Delta x}{2}
\end{equation}

We can improve things further by using a parabolic section (aka Simpson's rule):
\begin{equation}
    \int_a^b f(x) dx \approx \sum_{i=1}^N (f_i + 4f_{i+1/2}+f_{i+1}) \frac{\Delta x}{6}
\end{equation}

Let's see how these improve the estimate.

\begin{demo}[quad.cpp]
    Show how the error is much better with the trapezoid rule, and better still
    with Simpson's rule.  Have the class verify this with their function.
\end{demo}

Calculating the convergence rate for these quadrature rules is doable
analyticaly, but it's a bit tedious.  This is a good time to introduce the
\textit{numerical experiment}.  Let's find out how quickly these methods
converge empirically, by just trying a couple of different values of $N$:

\begin{demo}[converge.cpp]
    Demonstrate that the error for the rectangle rule is $\mathcal{O}(\Delta
    x)$, $\mathcal{O}(\Delta x^2)$ for the trapezoid rule, and
    $\mathcal{O}(\Delta x^4)$ for Simpson's rule.  Have the class verify this
    with their function.
\end{demo}

\subsection{Root Finding}
A common task we often find ourselves facing is trying to find the roots of a
function, that is the value of $x$ where
\begin{equation}
    f(x) = 0
\end{equation}

This problem is more useful than just the straight forward ``when's the function
equal to zero''.  We can use a root finder to find when two equations are equal,
as well as extrema (by looking at their derivatives).  These root finding
methods work in part by leveraging some of the numerical calculus tricks we've
built up previously.  All of these root finding techniques require either one or
two initial guesses, and will give us roots with a specified precision
(something often called ``tolerance'', our tolerance for error in other words)

In this section, we're going to examine a couple of different methods for
evaluating the root of a function.  To make things fun, let's pick a function
that doesn't have an analytic solution for the root:

\begin{equation}
    f(x) = e^x\ln(x)-x^2
\end{equation}

\subsubsection{Bisection Method}
The simplest root finding technique is the Bisection method, which is a kind of
``divide-and-conquer'' algorithm.  All we need for the bisection method is to
provide a value to the left and to the right of our root (bounding it, in other
words).  If we call these values $x_l$ and $x_r$, such that $f(x_l) < 0$ and
$f(x_r) > 0$, we can define a simple algorithm that will get us closer and
closer to the true root until we're within a given tolerance $\epsilon$.

The basic pseudocode of this method is as follows:
\begin{enumerate}
    \item Define a midpoint $x_m = \frac{x_l + x_r}{2}$.
    \item If $f(x_m) = 0 \pm \epsilon$, stop.  $x_m$ is the root.
    \item If $f(x_m) > 0$, then set $x_r = x_m$ and go to 1.
    \item If $f(x_m) < 0$, then set $x_l = x_m$ and go to 1.
\end{enumerate}

\begin{demo}[bisect.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~20 iterations or so.
    Show that decreasing the tolerance gives less iterations, and increasing it
    gives more.  Show that you can't go arbitrarily high.  Have the class try a
    different function.
\end{demo}
\subsubsection{Newton's Method}
The Bisection method is great, and you'll probably find yourself using it as
your default because it is very simple, and requires only some knowledge of the
vague area that the root might be in.  But we can make an algorithm that's a lot
faster if we know the derivative of the function.

With the derivative of a function, we can take a guess about a point
\textit{near} the root, and then calculate the derivative at that point.  By
linearly extrapolating to where that derivative is zero, we can iteratively
converge on a root a bit faster than with bisection. This method is called
Newton's Method, or the Newton-Raphson method.

Newton's method relies on making an initial guess as to what the root is.  This
guess $x_0$ may not be the true root, differing by some error $\delta x$ such
that $f(x_0+\delta x) = 0$.  If we Taylor expand this about $x_0$ and keep only
terms linear in $\delta x$, we get:
\begin{equation}
    \begin{aligned}
        f(x_0 + \delta x) & = f(x_0) + f'(x_0)\delta x + \mathcal{O}(\delta x^2) = 0\\
        0 &\approx f(x_0) + f'(x_0)\delta x
    \end{aligned}
\end{equation}

If you re-arrange this, you get $\delta x \approx -f(x_0)/f'(x_0)$.  With this,
we can build an \textit{iterative} method, replacing $x_0$ with $x_0 + \delta x$
until we are within our tolerance.

The basic pseudocode of this method is as follows:
\begin{enumerate}
    \item Take an initial guess $x_0$ that is near the root.
    \item If $f(x_0) = 0 \pm \epsilon$, stop.  $x_0$ is the root.
    \item Set $x_0 = x_0 - f(x_0)/f'(x_0)$ and go to 2.
\end{enumerate}

\begin{demo}[newton.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~4 iterations if we
    guess the midpoint of $[1,2]$, and ~6 if we use the edges.  Show that you
    can't go arbitrarily high.  Have the class try a different function.
\end{demo}

\subsubsection{Secant Method}
Newton's method is clearly faster at converging than the bisection method, but
it has the huge disadvantage of needing to know the derivative of a function.
Luckily, we have constructed a whole stable of schemes for calculating these
derivatives numerically.  We can just substitute these in to our algorithm in
place of the analytic derivative.  That's why the \textit{Secant method} is
sometimes called the \textit{discrete Newton method}.  

In Newton's method, we iterate $k$ times over our guess, $x_k$ and the value of
the function at this point $f_k$ and it's derivative $f'_k$.  The recursive
definition of this relationship is:
\begin{equation}
    x_{k+1} = x_k - \frac{f_k}{f'_k}
\end{equation}

We can replace the exact $f'_k$ with a finite difference approximation:
\begin{equation}
    f'_k \approx \frac{f_k - f_{k-1}}{x_k-x_{k-1}}
\end{equation}

This gives us the definition of the Secant method's guess:
\begin{equation}
    x_{k+1} = x_k - f_k\frac{x_k-x_{k-1}}{f_k-f_{k-1}}
\end{equation}

The algorithm is then essentially the same, with the added caveat that we need
to store the previous guess, and start with \textit{two} guesses.
\begin{enumerate}
    \item Take two initial guesses $x_0$ and $x_1$ that are near the root.
    \item If $f(x_0) = 0 \pm \epsilon$, stop.  $x_0$ is the root.
    \item Set $x_0 = x_0 - f(x_0)*(x_0-x_1)/(f(x_0)-f(x_1))$, and $x_1$ to be
        equal to the previous value of $x_0$.
    \item Go to 2.
\end{enumerate}

The Secant method doesn't converge quite as quickly as Newton's method, but it
doesn't require the calculation of an analytic derivative.

\begin{demo}[newton.cpp]
    Show that we converge to a tolerance of $10^{-6}$ in ~7 iterations if we
    use initial guesses of 1 and 2.  Have the class try a different function.
\end{demo}

\section{Ordinary Differential Equations}
One of the most common kinds of problems you'll face in your career as a
physicist isn't simple differentiation or integration tasks, but instead a set
of differential (or possibly integro-differential) equations.  One of the
simplest is the ubiquitous simple harmonic oscillator.  For a SHO, we have the
following differential equations we need to solve:

\begin{equation}
    \begin{aligned}
        \frac{dv}{dt} &= -\frac{k}{m}x \\
        \frac{dx}{dt} &= v \\
    \end{aligned}
\end{equation}

For this problem, we know the solution:

\begin{equation}
    x(t) = A\cos(\omega t - \phi)
\end{equation}

Where 
\begin{equation}
    \begin{aligned}
        \omega &= \sqrt{k/m} \\
        A &= \sqrt{x_0^2 + (v_0/\omega)^2}\\
        \phi &= \arctan{\frac{v_0}{\omega x_0}} 
    \end{aligned}
\end{equation}

With initial conditions $v_0$ and $x_0$.  

But what if we \textit{don't} know the solution?  In this chapter, we're going
to learn general strategies for solving Ordinary (that is, involving only
derivatives of a single variable) Differential Equations.  We'll start with the
most common and simple type, the \textit{initial value problem}.

\subsection{Initial Value Problem}
A huge fraction of physics problems either already are first-order ODEs, or can
be transformed \textit{into} first-order ODEs.  These equations have the general
form of

\begin{equation}
    \frac{d\mathbf{y}}{dt} = \mathbf{g}(\mathbf{y}, t)
\end{equation}

Where $\mathbf{y} = (y_1, y_2, y_n)$ are some set of $n$ dynamical variables,
and $\mathbf{g}$ is a generalized velocity that governs the time evolution of
that dynamical variable.  These need not be position and velocity, they could
just as easily be energy and power, mass and mass-flux, or angle and angular
velocity.

In an initial value problem, we solve the previous problem by evolving a system
forward from some $\mathbf{y}_0=\mathbf{y}|_{t=0}$.  For our example of a SHO,
we have a dynamical variable vector:

\begin{equation}
    \mathbf{y} = 
    \begin{pmatrix}
        y_1 \\
        y_2 \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        x \\
        v \\
    \end{pmatrix}
\end{equation}

With a generalized velocity vector:
\begin{equation}
    \mathbf{g} = 
    \begin{pmatrix}
        y_2 \\
        -\omega^2 y_1 \\
    \end{pmatrix}
\end{equation}

If we have values for $x_0$ and $v_0$, we can use these together with the
equations of motion above to solve for $x(t)$ and $v(t)$.

\subsection{The Euler and Picard Methods}
There are two ways of thinking about solving an IVP.  I generally find one of
these to be the ``intuitive'' way, and the other to be the ``rigorous'' way.
Let's start with the intuitive one.  For an IVP involving position and velocity
(a kinematics problem, for example), we initially know $x_0$ and $v_0$, and we
want to know $x(t)$ (or possibly $v(t)$).  So what we want to do is ``advance''
the values of $x$ and $v$ from $t=0\rightarrow t$.  How do we do that then?
Let's imagine the simplest case, where $a=0$, so $v(t)=v_0=\mathrm{const}$.  In
this case, our solution is obvious:

\begin{equation}
    x = x_0 + vt
\end{equation}

But what if $v$ is not constant?  Well, hopefully on \textit{some} scale, it is
at least smoothly varying.  If that's the case, then maybe what we can do is
treat $v$ as constant over a short interval $\Delta t$.  Then, we can advance
our position in small jumps:

\begin{equation}
    x_{i+1} = x_i + v_i \Delta t
\end{equation}

And we can do the same thing for the velocity:
\begin{equation}
    v_{i+1} = v_i + a_i \Delta t
\end{equation}

And then to get the whole thing up to time $t$, we start with $i=0$, and advance
it to $i=t/\Delta t$, iteratively updating x and v each time.  Essentially what
we're doing here is integrating these differential equations to transform:

\begin{equation}
    \begin{aligned}
        \int \frac{dx}{dt} &\rightarrow x(t) \\
        \int \frac{dv}{dt} &\rightarrow v(t) \\
    \end{aligned}
\end{equation}

This is why these methods for solving ODEs are often called ``numerical
integration''.  The method we've derived here is the simplest numerical
integration scheme, \textit{Euler's method}.

\begin{demo}[euler.cpp]
    Show that Euler's method works for a SHO.  Show what happens when the step
    size is made too large (unstable).
\end{demo}

You can see that Euler's method works reasonably well, but errors can accumulate
over time.  Shrinking the step size helps us to shrink these errors, thankfully,
but how quickly do they shrink?  Here we're going to need to introduce the more
``rigorous'' way of deriving Euler's method:

If we take again our old friend the Taylor expansion, we can expand $y(t+\Delta
t)$ about $t$:

\begin{equation}
    \begin{aligned}
        y(t+\Delta t) &= y(t) + y'(t)\Delta t + \mathcal{O}(\Delta t^2)\\
        y_{i+1} &= y_i + g_{i}\Delta t + \mathcal{O}(\Delta t^2)\\
    \end{aligned}
\end{equation}

This gives us our Euler method, with an error estimate!  The errors should be
$\mathcal{O}(\Delta t^2)$.  Let's check that they are:

\begin{demo}[euler\_converge.cpp]
    Show that the error is only decreasing linearly with smaller step size
    $\Delta t$.  Why is that?  (Remember number of steps required is $n\propto
    \Delta t^{-1}$)
\end{demo}

We've now seen that the Euler scheme's error is linear in the size of the
timestep.  We call this convergence behaviour \textit{first-order}: it depends
on $\Delta t^1$.  Let's construct a better algorithm, a second-order integrator.

We can do a bit better than Euler by combining our approximation of a derivative
with a better approximation of an integral.  It's formally true that:

\begin{equation}
    y_{i+1} = y_i + \int_{t_i}^{t_{i+1}} g(y,t) dt
\end{equation}

(Naturally, as what I've just written out is a re-phrasing of the Fundamental
Theorem of Calculus).

Euler's method approximates this integral as:
\begin{equation}
    \int_{t_i}^{t_{i+1}} g(y,t) dt \approx g_i\Delta t
\end{equation}

(In other words, the left-hand rectangle rule).  Maybe if we use a smarter
approximation of the integral, we can get a better scheme.  Let's try the
Trapezoid rule:

\begin{equation}
    \int_{t_i}^{t_{i+1}} g(y,t) dt \approx \frac{\Delta t}{2}(g_i + g_{i+1})
\end{equation}

This should give us smaller errors than the previous case, because the trapezoid
rule has errors $\mathcal{O}(\Delta t^3)$, which should give a final scheme with
error $\mathcal{O}(\Delta t^2)$.

So now we have a scheme that looks like this:
\begin{equation}
    y_{i+1} = y_i + \frac{\Delta t}{2}(g_i+g_{i+1}) + \mathcal{O}(\Delta t^3)
\end{equation}

All well and good, except for one big question:  HOW DO WE CALCULATE SOMETHING
THAT DEPENDS ON A FUTURE VALUE?  We need $g_{i+1}$ to calculate $y_{i+1}$, but
$g_{i+1}$ presumably depends on $y_{i+1}$.  What do we do here?

What we have here is called an \textit{implicit} method/problem.  The difference
between these two methods is that an \textit{explicit} method involves finding
some function $f(y)$ such that:

\begin{equation}
    y_{i+1} = A(y_i)
\end{equation}

Whereas an implicit method instead requires you to find a root of an equation
such that:

\begin{equation}
    B(y_i, y_{i+1}) = 0
\end{equation}

In other words, an optimization or root-finding problem.  The root we need to
find is:

\begin{equation}
    y_{i+1} - y_i - \frac{\Delta t}{2}(g_i+g_{i+1}) = 0
\end{equation}

Solving for the roots $y_{i+1}$ and $g_{i+1}$ give us the next value at a given
time.  We can either use our previously derived root-finding tools for this, or
we can use a clever method called \textit{Fixed-Point Iteration}.  The idea
behind this is that we are looking to find a point $x_{fix}$ for some function,
such that:

\begin{equation}
    f(x_{fix}) = x_{fix}
\end{equation}

In our case, the fixed point is $y_{i+1}$, and the function is:

\begin{equation}
    y_{i+1} = y_i + \frac{\Delta t}{2}(g(y_i)+g(y_{i+1}))
\end{equation}

We can solve for this using one of many methods, but the easiest is almost
certainly \textit{Picard iteration}.  With this, we calculate new values for the
vector $y_{i+1}$ over and over again, eventually stopping after a fixed number
of iterations (bad! lazy!) or once a certain error tolerance has been reached.
This is defined simply as:
\begin{equation}
    \begin{aligned}
        y^{0}_{i+1} &= y_i \\
        y^{(k+1)}_{i+1} &= y_i + \frac{\Delta t}{2}(g(y_i)+g(y^{(k)}_{i+1}))
    \end{aligned}
\end{equation}

Let's see how this works with a very simple ODE:

\begin{equation}
    \frac{dy}{dt} = g = y
\end{equation}

This gives us:

\begin{equation}
    y_{i+1} = y_i + \frac{\Delta t}{2}(y_i+y_{i+1})
\end{equation}

With an initial value of $y(0) = 1$, the solution for our ODE is

\begin{equation}
    y(t) = e^t
\end{equation}

So $y(0.1)$ should be exactly $1.1051709$.  Let's see if we can Picard iterate
towards this value.  We will start from $t=0$ and go to $t=0.1$ in one big step
of $\Delta t = 0.1$.  Our initial condition will therefore be $y_i=1$, and our
initial guess will be $y^{(0)}_{i+1}=1$.  Let's see if it works.

\begin{equation}
    \begin{aligned}
        y^{(k+1)}_{i+1} &= y_i + \frac{\Delta t}{2}(g(y_i)+g(y^{(k)}_{i+1})) \\
        y^{(1)}_{i+1} &= 1 + 0.05(1+1) &= 1.01 \\
        y^{(2)}_{i+1} &= 1 + 0.05(1+1.01) &= 1.1005 \\
        y^{(3)}_{i+1} &= 1 + 0.05(1+1.1005) &= 1.105025 \\
        y^{(4)}_{i+1} &= 1 + 0.05(1+1.105025) &= 1.1052625 \\
        y^{(5)}_{i+1} &= 1 + 0.05(1+1.1052625) &= 1.10526313 \\
        y^{(6)}_{i+1} &= 1 + 0.05(1+1.10526313) &= 1.10526315 \\
    \end{aligned}
\end{equation}

You can see we've converged, but not exactly to the true value.  Remember that
we're dealing with a truncated result here: there is a non-zero error term that
just iterating away at won't help.  Let's see how well this Picard method works

\begin{demo}[picard.cpp]
    Show how much more stable the picard method is than the Euler method.  Try
    dropping the step sizes and see what happens.
\end{demo}

You've noticed this thing is \textit{way} more stable than Euler's method with
large-ish step sizes?  That's one of the huge advantages of implicit methods:
they tend to either fail utterly (like if the iteration doesn't converge) or
give a stable result.  You don't have to worry quite as much as with an
explicit method if you are going to get a result that's wrong because of a
numerical instability that's grown.  Let's see how the convergence looks.  Based
on the truncation error of $\mathcal{O}(\Delta t^3)$, the error should converge
as $\mathcal{O}(\Delta t^2)$: the method is \textit{second-order}.

\begin{demo}[picard\_converge.cpp]
    Show that the picard method is second order, but note that it is slow.  It
    takes a long time to do all those extra iterations (in our lazy case, 10
    times as much time).
\end{demo}

So we've seen now a semi-implicit method (since we're only doing implicit
calculations for intermediate steps, this is also called an IMEX or
IMplicit-EXplicit method).  Unfortuantely, all that extra iterating is
expensive.  Maybe there's a way to skip that iteration?

\subsection{Predictor-Corrector Methods}
It turns out, yes indeed there is a way to skip that iteration!  The trick is to
use a cheap, explicit first guess (like, say from the Euler method).  This
``prediction'' is then corrected using an implicit method, like the single
Picard step we saw previously.  We will first produce a
weak prediction for $\tilde y_{i+1}$:

\begin{equation}
    \tilde y_{i+1} = y_i + g_i\Delta t
\end{equation}

Which we then correct to a better estimate of $y_{i+1}$:

\begin{equation}
    y_{i+1} = y_i + \frac{\Delta t}{2}(g_i+\tilde g_{i+1})
\end{equation}

This is actually just one of many classes of ODE solvers called
``predictor-corrector'' methods.  Let's see how well it works. 
\begin{demo}[pred\_corr.cpp]
    Show that the predictor corrector is also quite stable.  Walk through the
    code.
\end{demo}

Predictor-Corrector methods can be of any order, as long as you
include enough terms.  The one we have here is second-order.  Let's verify that,
and also check that it's faster than Picard Iteration.

\begin{demo}[pc\_converge.cpp]
    Show that this is also second order, and that it's much faster than Picard.
\end{demo}

\subsection{Runge-Kutta Methods}
One of the powerful ways of building numerical (read: approximate) methods is,
as we've kept seeing, to use combinations of Taylor series approximations to
cancel out high-order error terms and leave us with a more accurate,
faster-converging method.  A general scheme was developed around the turn of the
1900s by two German mathematicians, Carl Runge and Wilhelm Kutta.  The Pang
textbook has an extremely oblique and confusing derivation of this method, so I
hope I can do a bit better.

To understand the family of Runge-Kutta methods, we're going to start out by
constructing a method that's not much better than the Predictor-Corrector or
Picard method (it's second order), but is easy to understand.  We'll start again
by expanding $y$ about $t$ and evaluating it at $t+\Delta t$:

\begin{equation}
    y(t+\Delta t) = y(t)+\Delta ty'(t) + \frac{\Delta t^2}{2}y''(t) +
    \mathcal{O}(\Delta t^3)
\end{equation}

If our derivative vector is $g(t,y) = y'(t)$, then this gives us:

\begin{equation}
    y(t+\Delta t) = y(t)+\Delta tg(t,y(t)) + \frac{\Delta t^2}{2} \frac{d}{dt}
    g(t,y(t)) + \mathcal{O}(\Delta t^3)
\end{equation}

Because $g$ is a multivariable function, we need to apply the chain rule here to
expand it:

\begin{equation}
    \frac{dg}{dt} = \frac{\partial g}{\partial t}\frac{\partial t}{\partial t} +
    \frac{\partial g}{\partial y}\frac{\partial y}{\partial t} = g_t + g_y g
\end{equation}

More formally, $\frac{\partial g}{\partial y}$ should be the Jacobian of $g$,
but we're lazy, so let's just pretend we've only got a scalar $y$.

If we then plug this into our Taylor expansion, we get:
\begin{equation}
    y(t+\Delta t) = y(t)+\Delta tg(t,y(t)) + \frac{\Delta t^2}{2}(g_t(t,y) +
    g(t,y)g_y(t,y)) + \mathcal{O}(\Delta t^3)
\end{equation}

Nothing new or too radical so far.  What we need to do now is introduce a second
Taylor series expansion, this time expanding not $y$ but instead $g$.  However,
$g$ is a multivariable function!  This means we need to introduce the
multivariate Taylor Series.

For a single variable, the Taylor series of a function $f$ expanded about $a$ is
defined as:
\begin{equation}
    f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!}(x-a)^n
\end{equation}

Where $f^{(n)}$ is the n-th order derivative of $f$.  But what do we have if $f$
depends on multiple variables?  Generally, it's a horrible mess!  I don't want
to even write it down it's such an ugly expression, look it up online if you
want to hurt your eyes.

We, unfortunately, still need to use it.  The low-order terms are pretty
reasonable and easy to understand.  Let's do that for our $g(t,y)$:

\begin{equation}
    g(t+\Delta t, y+\Delta y) = g(t,y) + g_t(t,y) \Delta t + g_y(t,y) \Delta y +
    ...
\end{equation}

If we let $\Delta y = g(t,y)\Delta t$, then we get:
\begin{equation}
    g(t+\Delta t, y+g(t,y)\Delta t) = g(t,y) + g_t(t,y) \Delta t + g_y(t,y)
    g(t,y) \Delta t + \mathcal{O}(\Delta t^2)
\end{equation}

If we re-arrange our Taylor expansion, we can see something that gives us a hint
as to what we can do with this:
\begin{equation}
    y(t+\Delta t) = y(t)+\frac{\Delta t}{2}g(t,y) + \frac{\Delta t}{2}(g(t,y) +
    g_t(t,y)\Delta t + g(t,y)g_y(t,y)\Delta t) + \mathcal{O}(\Delta t^3)
\end{equation}

The last term we didn't omit is the same as the first few terms of the Taylor
expansion for $g(t+\Delta t, y+\Delta y)$! So what we have is:

\begin{equation}
    \begin{aligned}
        y(t+\Delta t) &= y(t)+\frac{\Delta t}{2}g(t,y) + \frac{\Delta
    t}{2}\left[g(t+\Delta t, y + g\Delta t) + \mathcal{O}(\Delta t^2)\right] +
    \mathcal{O}(\Delta t^3)\\
        &= y(t)+\frac{\Delta t}{2}\left[g(t,y) + g(t+\Delta t, y + g\Delta t) \right] +
    \mathcal{O}(\Delta t^3)\\
    \end{aligned}
\end{equation}

This means, when we have $N\propto t^{-1}$ steps, the order of this method will
be $\mathcal{O}(\Delta t^2)$.  This method is called the second-order
Runge-Kutta method, or sometimes Heun's method.  Putting it into our usual
notation, it gives us:

\begin{equation}
    y_{n+1} = y_n + \Delta t(\frac{1}{2}k_1 + \frac{1}{2} k_2)
\end{equation}

Where
\begin{equation}
    \begin{aligned}
        k_1 &= g(t_n, y_n) \\
        k_2 &= g(t_n + \Delta t, y_n + k_1\Delta t) \\
    \end{aligned}
\end{equation}

Let's see if this works!
\begin{demo}[rk2.cpp]
    Show that the 2nd order RK method works, but has a slight phase offset.
\end{demo}

It should also be second order, let's check that it is:
\begin{demo}[rk2\_converge.cpp]
    Show that the 2nd order RK method is second order.
\end{demo}

Runge-Kutta methods are actually a family of methods, which all have the form:

\begin{equation}
    y_{n+1} = y_n + \Delta t\sum_{i=1}^m b_i k_i
\end{equation}

Where the terms $k_i$ are defined recursively:

\begin{equation}
    \begin{aligned}
        k_1 &= g(t_n, y_n) \\
        k_i &= g(t_n + c_i\Delta t, y_n + \Delta t\sum_{j=1}^{i-1}a_{ij}k_j)
    \end{aligned}
\end{equation}

For the second-order RK method, we have $c_2=1$, $a_{21}=1$, and $b_1=b_2=1/2$.
A convenient way of keeping track of these coefficients is the \textit{Butcher
tableau}, a notation developed by John C. Butcher in the 1970s.  The Butcher
Tableau lays out the coefficients like a matrix, which for explicit RK methods
is lower-triangular.  The Tableau looks like this:

\[
\begin{array}{c|ccccc}
    0\\
    c_2 & a_{21} \\
    c_3 & a_{31} & a_{32} \\
    \vdots & \vdots & \ddots \\
    c_m & a_{m1} & a_{m2} & \dots & a_{m,m-1} \\
    \hline
    & b_i & b_2 & \dots &  \dots & b_m \\
\end{array}
\]

We have near-total freedom to choose these parameters, except for one critical
requirement:

\begin{equation}
    \sum_{i=1}^m b_i = 1
\end{equation} 

That requirement is needed for the method to be consistent with the Taylor
series expansions that constructed it.  The choice of the other terms, and the
total number of terms to include, sets the order of the method. The RK2 method
has a Butcher Tableau of:

\[
\begin{array}{c|cccc}
    0\\
    1 & 1 \\
    \hline
    & 1/2 & 1/2 
\end{array}
\]

I won't derive it, because it's an ungodly tedious pile of work, but there is a
high order $\mathcal{O}(\Delta t^4)$ RK method that is often referred to as
``the'' Runge-Kutta Method.  RK4 has a Butcher tableau of:

\[
\begin{array}{c|cccc}
    0\\
    1/2 & 1/2 \\
    1/2 & 0 & 1/2 \\
    1 & 0 & 0 & 1 \\
    \hline
    & 1/6 & 1/3 & 1/3 & 1/6
\end{array}
\]

In general, if you are faced with an ODE that needs solving, the RK4 method
should be the first tool you pull out.  It may not be the best choice for every
problem, but it is generally the most likely to give you a good answer on your
first try.  Let's see it in action.

\begin{demo}[rk4.cpp]
    Show that the 4nd order RK method works great.
\end{demo}

It should also be FOURTH order, let's check that it is:
\begin{demo}[rk4\_converge.cpp]
    Show that the 4nd order RK method is fourth order.
\end{demo}

\subsection{Symplectic Integrators and Energy Conservation}
So far the only criterion we've considered for whether a method for solving an
ODE is ``good'' or not is the cost per timestep $\Delta t$ as well as the
convergence: how quickly the error fell as $\Delta t$ is decreased.  However,
we saw with our example of Euler's method that the total energy actually was
increasing monotonically, which is way worse than just a random error, it's a
systematic one in a property we care about (total energy).  

It turns out that even our good friend RK4 suffers from energy non-conservation.
In our simple harmonic oscillator, the total energy (Hamiltonian) of the system
is:

\begin{equation}
    H = T + V = \frac{1}{2}(p^2 + q^2)
    \label{SHO_Hamiltonian}
\end{equation}

If we choose units $m=\omega=1$, then $x=q$ and $v=p$, which will simplify some
of our derivations.  Let's see how well the energy is conserved with a SHO and
RK4.

\begin{demo}[rk4\_energy.cpp]
    Show that the total energy of an RK4-integrated SHO decreases systematically.
\end{demo}

That's not good!  If we are dealing with a problem like an SHO, and we have many
periods/dynamical times, even RK4 is going to become a problem.  This is
extremely relevant in astrophysical problems, where things like planets can
make billions of orbits around a star during their lifetime.  We can see how bad
this can turn out by looking at an object in orbit around a point mass.  We'll
keep everything simple, and use two-dimensions.  The equations of motion for
this problem will be:

\begin{equation}
    \frac{d^2 \vec r}{dt} = -\frac{1}{r^2} \hat r
\end{equation}

If we start with a particle orbiting at $|r|=1$ with $|v|=1$ and $\vec r \cdot
\vec v = 0$, the particle should stay on an orbit with $r=1$ forever.  Let's see
what happens with the best and worst case integrators we've seen so far.

\begin{demo}[orbits.cpp]
    Show that both Euler's method and RK4 do very bad at keeping a planet on
    its orbit.
\end{demo}

Are we just screwed?  Is there any way to fix this problem?  To find out, we
should first try and understand the source of the problem.  We might think that
it's just the truncation error, but if that was the case than why is it
systematic? Let's see if we can find the source for the simpler case, our
Euler's method.

Recall Hamilton's equations:

\begin{equation}
    \frac{dq}{dt} = \frac{\partial H}{\partial p} \qquad \frac{dp}{dt} =
    -\frac{\partial H}{\partial q}
\end{equation}

So for the SHO, our equations of motion become the familiar ones we've seen
since Physics I:
\begin{equation}
    q' = p \qquad p' = - q
\end{equation}

So our Euler update step for $p$ and $q$ will be: 

\begin{equation}
    \begin{aligned}
        q_{n+1} & = q_n + \Delta t p_n \\
        p_{n+1} & = p_n - \Delta t q_n \\
    \end{aligned}
\end{equation}

Let's see what happens when we calculate $H_{n+1}$:
\begin{equation}
    \begin{aligned}
        H_{n+1} & = \frac{1}{2}(p_{n+1}^2 + q_{n+1}^2) =
        \frac{1}{2}\left[\left(p_n -
        \Delta t q_n\right)^2 + \left(q_n + \Delta t p_n\right)^2 \right]\\
        &= \frac{1}{2}\left[(p_n^2 -2\Delta t p_n q_n + \Delta t^2 q_n^2) +
        (q_n^2 + 2\Delta p_n q_n + \Delta t^2 p_n^2) \right] \\
        &= \frac{1}{2}(1+\Delta t^2)(p_n^2 + q_n^2) \neq H_n
    \end{aligned}
\end{equation}

Well there we have it, the energy's not conserved because our update scheme
explicitly does not conserve it!  The energy errors go down as we decrease the
timestep, but they are monotonically increasing.  Euler's method, for a SHO,
will always have the energy increase.

\end{document}
